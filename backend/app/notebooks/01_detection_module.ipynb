{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad64bbb1",
   "metadata": {},
   "source": "# 01. Módulo de Detección - Arquitectura Híbrida (Kornia + YOLO)\n\nEste notebook implementa el sistema de detección y seguimiento de objetos para OccultaShield.\n\n## Arquitectura de Detección\n\n```\n┌─────────────────┬───────────────────────┬─────────────────┐\n│ Tipo            │ Modelo                │ Framework       │\n├─────────────────┼───────────────────────┼─────────────────┤\n│ Caras           │ YuNet (FaceDetector)  │ Kornia AI       │\n│ Personas        │ YOLOv10 (nano/s/m)    │ Ultralytics     │\n│ Matrículas      │ YOLO-LPR              │ Ultralytics     │\n└─────────────────┴───────────────────────┴─────────────────┘\n```\n\n**Ventajas de esta arquitectura:**\n- Kornia FaceDetector (YuNet) es nativo GPU - sin dependencias adicionales\n- YOLOv10 para personas - precisión comprobada + velocidad\n- Auto-adaptación según VRAM disponible (nano/small/medium)"
  },
  {
   "cell_type": "markdown",
   "id": "da7ccea8",
   "metadata": {},
   "source": [
    "### 1. Imports y Configuración Inicial\n",
    "Aquí preparamos el entorno:\n",
    "- Importamos librerías clave: `cv2` (procesamiento de video), `numpy` (matrices), `torch` y `ultralytics` (IA/YOLO).\n",
    "- `nest_asyncio.apply()`: Es fundamental en Jupyter para permitir que bucles asíncronos (async/await) funcionen dentro de las celdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198204a8",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport asyncio\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Optional, Tuple, Callable\nfrom dataclasses import dataclass, field, asdict\nfrom enum import Enum\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport cv2\nimport numpy as np\nimport torch\nimport nest_asyncio\nfrom ultralytics import YOLO\nfrom scipy.optimize import linear_sum_assignment\n\n# Kornia para efectos GPU y detección de caras\ntry:\n    import kornia\n    import kornia.filters\n    from kornia.contrib import FaceDetector, FaceDetectorResult\n    KORNIA_AVAILABLE = True\n    KORNIA_FACE_AVAILABLE = True\nexcept ImportError:\n    try:\n        import kornia\n        import kornia.filters\n        KORNIA_AVAILABLE = True\n        KORNIA_FACE_AVAILABLE = False\n        print(\"Kornia instalado pero FaceDetector no disponible. Actualiza: pip install kornia>=0.7.0\")\n    except ImportError:\n        KORNIA_AVAILABLE = False\n        KORNIA_FACE_AVAILABLE = False\n        print(\"Kornia no instalado. Ejecuta: pip install kornia>=0.7.0\")\n\nnest_asyncio.apply()\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('detection_module')\n\n# =============================================================================\n# CONFIGURACIÓN\n# =============================================================================\nMIN_DETECTION_AREA = 500  # Área mínima (más pequeña para caras/matrículas)\n\nclass PrivacyCategory(Enum):\n    \"\"\"Categorías de datos sensibles según GDPR\"\"\"\n    PERSON = \"person\"\n    FACE = \"face\"\n    LICENSE_PLATE = \"license_plate\"\n    FINGERPRINT = \"fingerprint\"\n    ID_DOCUMENT = \"id_document\"\n    CREDIT_CARD = \"credit_card\"\n    SIGNATURE = \"signature\"\n\n# Mapeo de severidad GDPR por tipo de detección\nGDPR_SEVERITY = {\n    \"face\": \"high\",\n    \"fingerprint\": \"high\",\n    \"license_plate\": \"high\",\n    \"id_document\": \"high\",\n    \"credit_card\": \"high\",\n    \"signature\": \"medium\",\n    \"person\": \"medium\",\n}\n\n# =============================================================================\n# GPU MANAGER - Auto-detección y gestión de VRAM\n# =============================================================================\nclass GPUManager:\n    \"\"\"\n    Singleton para gestionar recursos GPU.\n    Detecta automáticamente la GPU y su VRAM disponible.\n    \"\"\"\n    _instance = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialized = False\n        return cls._instance\n    \n    def __init__(self):\n        if self._initialized:\n            return\n        self._initialized = True\n        self._refresh_info()\n    \n    def _refresh_info(self):\n        if torch.cuda.is_available():\n            self.device = \"cuda\"\n            self.device_name = torch.cuda.get_device_name(0)\n            self.vram_total_mb = torch.cuda.get_device_properties(0).total_memory // (1024**2)\n            self.vram_free_mb = self._get_free_vram()\n        else:\n            self.device = \"cpu\"\n            self.device_name = \"CPU\"\n            self.vram_total_mb = 0\n            self.vram_free_mb = 0\n        \n        logger.info(f\"GPU Manager: {self.device_name}, VRAM: {self.vram_total_mb}MB\")\n    \n    def _get_free_vram(self) -> int:\n        if not torch.cuda.is_available():\n            return 0\n        torch.cuda.synchronize()\n        allocated = torch.cuda.memory_allocated(0)\n        total = torch.cuda.get_device_properties(0).total_memory\n        return (total - allocated) // (1024**2)\n    \n    def can_fit_model(self, required_mb: int, safety_margin: float = 0.2) -> bool:\n        \"\"\"Verifica si hay suficiente VRAM para cargar un modelo\"\"\"\n        if self.device == \"cpu\":\n            return True\n        available = self._get_free_vram()\n        required_with_margin = int(required_mb * (1 + safety_margin))\n        return available >= required_with_margin\n    \n    def get_strategy(self) -> Tuple[str, str, int]:\n        \"\"\"\n        Determina la estrategia óptima según VRAM disponible.\n        Returns: (strategy, model_size, batch_size)\n        \"\"\"\n        vram_gb = self.vram_total_mb / 1024\n        \n        if vram_gb < 8:\n            return \"sequential\", \"nano\", 4\n        elif vram_gb < 16:\n            return \"parallel\", \"small\", 16\n        else:  # 16GB+ → usar medium (máximo permitido) con batch escalable\n            return \"parallel\", \"medium\", min(64, int(vram_gb * 2))\n\n# Instancia global\ngpu_manager = GPUManager()\n\n# =============================================================================\n# HYBRID DETECTOR MANAGER - Kornia FaceDetector + YOLOv10\n# =============================================================================\nclass HybridDetectorManager:\n    \"\"\"\n    Gestor híbrido de detectores: Kornia AI (caras) + YOLO (personas, matrículas).\n    \n    Arquitectura:\n    - Personas: YOLOv10 (nano/s/m según VRAM)\n    - Caras: Kornia YuNet (FaceDetector) - nativo GPU, sin dependencias extra\n    - Matrículas: YOLO-LPR\n    \n    Ventajas:\n    - Reduce dependencia de modelos YOLO de 3 a 2\n    - FaceDetector de Kornia es nativo GPU y muy eficiente\n    - Mantiene precisión de YOLOv10 para personas\n    \"\"\"\n    \n    # Configuración de modelos YOLO por tamaño\n    YOLO_CONFIGS = {\n        \"nano\": {\"person\": \"yolov10n.pt\", \"plate\": \"yolov8n.pt\"},\n        \"small\": {\"person\": \"yolov10s.pt\", \"plate\": \"yolov8s.pt\"},\n        \"medium\": {\"person\": \"yolov10m.pt\", \"plate\": \"yolov8m.pt\"},\n    }\n    \n    def __init__(\n        self, \n        gpu_mgr: GPUManager = None,\n        person_model: str = None,\n        plate_model: str = None,\n        face_confidence: float = 0.5,\n        person_confidence: float = 0.5\n    ):\n        \"\"\"\n        Args:\n            gpu_mgr: Instancia de GPUManager (usa global si no se proporciona)\n            person_model: Ruta personalizada al modelo de personas\n            plate_model: Ruta personalizada al modelo de matrículas\n            face_confidence: Umbral de confianza para caras (Kornia)\n            person_confidence: Umbral de confianza para personas/matrículas (YOLO)\n        \"\"\"\n        self.gpu = gpu_mgr or gpu_manager\n        self.device = self.gpu.device\n        self.strategy, self.model_size, self.batch_size = self.gpu.get_strategy()\n        \n        self.face_confidence = face_confidence\n        self.person_confidence = person_confidence\n        \n        # Inicializar detectores\n        self._init_face_detector()\n        self._init_yolo_detectors(person_model, plate_model)\n        \n        logger.info(f\"HybridDetectorManager: strategy={self.strategy}, size={self.model_size}, \"\n                   f\"device={self.device}, kornia_face={KORNIA_FACE_AVAILABLE}\")\n    \n    def _init_face_detector(self):\n        \"\"\"Inicializa Kornia FaceDetector (YuNet)\"\"\"\n        self.face_detector = None\n        \n        if KORNIA_FACE_AVAILABLE:\n            try:\n                self.face_detector = FaceDetector().to(self.device)\n                logger.info(\"✓ Kornia FaceDetector (YuNet) loaded\")\n            except Exception as e:\n                logger.warning(f\"Could not load Kornia FaceDetector: {e}\")\n                self.face_detector = None\n        \n        # Fallback a OpenCV Haar si Kornia no está disponible\n        if self.face_detector is None:\n            self.face_cascade = cv2.CascadeClassifier(\n                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n            )\n            logger.info(\"Using OpenCV Haar Cascade fallback for face detection\")\n    \n    def _init_yolo_detectors(self, person_model: str, plate_model: str):\n        \"\"\"Inicializa detectores YOLO para personas y matrículas\"\"\"\n        config = self.YOLO_CONFIGS[self.model_size]\n        \n        # Detector de personas (YOLOv10)\n        person_path = person_model or config[\"person\"]\n        try:\n            self.person_detector = YOLO(person_path)\n            logger.info(f\"✓ YOLO person detector loaded: {person_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to load person model: {e}\")\n            self.person_detector = None\n        \n        # Detector de matrículas (opcional)\n        self.plate_detector = None\n        if plate_model and os.path.exists(plate_model):\n            try:\n                self.plate_detector = YOLO(plate_model)\n                logger.info(f\"✓ YOLO plate detector loaded: {plate_model}\")\n            except Exception as e:\n                logger.warning(f\"Could not load plate model: {e}\")\n    \n    def _numpy_to_tensor(self, frame: np.ndarray) -> torch.Tensor:\n        \"\"\"Convierte frame numpy BGR a tensor GPU RGB normalizado\"\"\"\n        # BGR -> RGB\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # (H, W, C) -> (1, C, H, W) normalizado a [0, 1]\n        tensor = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n        return tensor.to(self.device)\n    \n    def detect_faces_kornia(self, tensor: torch.Tensor, frame_num: int) -> List[Tuple[str, 'BoundingBox']]:\n        \"\"\"Detecta caras usando Kornia FaceDetector (YuNet)\"\"\"\n        if self.face_detector is None:\n            return []\n        \n        results = []\n        \n        with torch.no_grad():\n            detections = self.face_detector(tensor)\n        \n        # Procesar resultados de Kornia FaceDetector\n        for det in detections:\n            if det.data.numel() == 0:\n                continue\n            \n            # YuNet devuelve [x, y, w, h, score, landmarks...]\n            # get_keypoints() devuelve keypoints, usamos los primeros 4 para bbox\n            try:\n                # Obtener bboxes y scores\n                data = det.data.cpu().numpy()\n                for row in data:\n                    if len(row) >= 5:\n                        x, y, w, h, score = row[:5]\n                        \n                        if score >= self.face_confidence:\n                            # Desnormalizar coordenadas si están normalizadas\n                            _, _, img_h, img_w = tensor.shape\n                            \n                            # Convertir a x1, y1, x2, y2\n                            x1, y1 = float(x), float(y)\n                            x2, y2 = float(x + w), float(y + h)\n                            \n                            bbox = BoundingBox(x1, y1, x2, y2, float(score), frame_num)\n                            \n                            if bbox.area >= MIN_DETECTION_AREA:\n                                results.append((\"face\", bbox))\n            except Exception as e:\n                logger.debug(f\"Error processing face detection: {e}\")\n        \n        return results\n    \n    def detect_faces_opencv(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, 'BoundingBox']]:\n        \"\"\"Fallback: Detección de caras con OpenCV Haar Cascade\"\"\"\n        if not hasattr(self, 'face_cascade'):\n            return []\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        \n        faces = self.face_cascade.detectMultiScale(\n            gray,\n            scaleFactor=1.1,\n            minNeighbors=5,\n            minSize=(30, 30)\n        )\n        \n        results = []\n        for (x, y, w, h) in faces:\n            bbox = BoundingBox(\n                x1=float(x), \n                y1=float(y), \n                x2=float(x + w), \n                y2=float(y + h),\n                confidence=0.8,  # Haar no da confianza real\n                frame=frame_num\n            )\n            if bbox.area >= MIN_DETECTION_AREA:\n                results.append((\"face\", bbox))\n        \n        return results\n    \n    def detect_persons(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, 'BoundingBox']]:\n        \"\"\"Detecta personas usando YOLOv10\"\"\"\n        if self.person_detector is None:\n            return []\n        \n        results_yolo = self.person_detector.predict(\n            frame, \n            conf=self.person_confidence, \n            verbose=False,\n            device=self.device\n        )\n        \n        results = []\n        for r in results_yolo:\n            for box in r.boxes:\n                cls = int(box.cls[0])\n                if cls == 0:  # COCO class 0 = person\n                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n                    bbox = BoundingBox(x1, y1, x2, y2, float(box.conf[0]), frame_num)\n                    \n                    if bbox.area >= MIN_DETECTION_AREA:\n                        results.append((\"person\", bbox))\n        \n        return results\n    \n    def detect_plates(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, 'BoundingBox']]:\n        \"\"\"Detecta matrículas usando YOLO-LPR\"\"\"\n        if self.plate_detector is None:\n            return []\n        \n        results_yolo = self.plate_detector.predict(\n            frame, \n            conf=self.person_confidence, \n            verbose=False,\n            device=self.device\n        )\n        \n        results = []\n        for r in results_yolo:\n            for box in r.boxes:\n                x1, y1, x2, y2 = box.xyxy[0].tolist()\n                bbox = BoundingBox(x1, y1, x2, y2, float(box.conf[0]), frame_num)\n                \n                if bbox.area >= MIN_DETECTION_AREA:\n                    results.append((\"license_plate\", bbox))\n        \n        return results\n    \n    async def detect_all(\n        self, \n        frames: List[np.ndarray], \n        frame_nums: List[int],\n        conf_threshold: float = 0.5\n    ) -> Dict[int, List[Tuple[str, 'BoundingBox']]]:\n        \"\"\"\n        Ejecuta todos los detectores en los frames proporcionados.\n        \n        Args:\n            frames: Lista de frames numpy BGR\n            frame_nums: Números de frame correspondientes\n            conf_threshold: Umbral de confianza\n            \n        Returns:\n            Dict[frame_num, List[(label, BoundingBox)]]\n        \"\"\"\n        all_detections = {fn: [] for fn in frame_nums}\n        \n        for frame, frame_num in zip(frames, frame_nums):\n            # Detección de personas (YOLO)\n            persons = self.detect_persons(frame, frame_num)\n            all_detections[frame_num].extend(persons)\n            \n            # Detección de caras (Kornia o OpenCV fallback)\n            if self.face_detector is not None:\n                tensor = self._numpy_to_tensor(frame)\n                faces = self.detect_faces_kornia(tensor, frame_num)\n            else:\n                faces = self.detect_faces_opencv(frame, frame_num)\n            all_detections[frame_num].extend(faces)\n            \n            # Detección de matrículas (YOLO)\n            plates = self.detect_plates(frame, frame_num)\n            all_detections[frame_num].extend(plates)\n        \n        return all_detections\n    \n    def get_info(self) -> Dict:\n        \"\"\"Retorna información sobre la configuración actual\"\"\"\n        detectors = []\n        if self.person_detector:\n            detectors.append(\"person (YOLOv10)\")\n        if self.face_detector:\n            detectors.append(\"face (Kornia YuNet)\")\n        elif hasattr(self, 'face_cascade'):\n            detectors.append(\"face (OpenCV Haar)\")\n        if self.plate_detector:\n            detectors.append(\"plate (YOLO)\")\n        \n        return {\n            \"strategy\": self.strategy,\n            \"model_size\": self.model_size,\n            \"batch_size\": self.batch_size,\n            \"device\": self.device,\n            \"vram_total_mb\": self.gpu.vram_total_mb,\n            \"detectors\": detectors,\n            \"kornia_available\": KORNIA_FACE_AVAILABLE\n        }\n\nprint(f\"✓ GPU detectada: {gpu_manager.device_name}\")\nprint(f\"✓ VRAM total: {gpu_manager.vram_total_mb}MB\")\nprint(f\"✓ Estrategia recomendada: {gpu_manager.get_strategy()}\")\nprint(f\"✓ Kornia FaceDetector disponible: {KORNIA_FACE_AVAILABLE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "449c82c0",
   "metadata": {},
   "source": [
    "### 2. Modelos de Datos (`models.py`)\n",
    "Definimos las estructuras de datos (clases) que usaremos para organizar la información:\n",
    "- **`BoundingBox`**: Representa un recuadro detectado (coordenadas x1, y1, x2, y2). Incluye métodos útiles para calcular su área, ancho y alto.\n",
    "- **`Capture`**: Guarda la información de una \"foto\" (snapshot) tomada a un objeto (ruta del archivo, frame, motivo...).\n",
    "- **`TrackedDetection`**: Es el historial completo de un objeto único a lo largo del tiempo. Contiene su ID, todas sus posiciones pasadas (`bbox_history`) y sus capturas.\n",
    "- **`DetectionResult`**: Es el informe final del procesamiento del video. Resume estadísticas (FPS, duración) y contiene la lista de todas las detecciones encontradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e1d07",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass BoundingBox:\n    x1: float\n    y1: float\n    x2: float\n    y2: float\n    confidence: float\n    frame: int\n\n    @property\n    def width(self) -> float:\n        return self.x2 - self.x1\n\n    @property\n    def height(self) -> float:\n        return self.y2 - self.y1\n\n    @property\n    def area(self) -> float:\n        return self.width * self.height\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n@dataclass\nclass Capture:\n    frame: int\n    image_path: str\n    bbox: BoundingBox\n    reason: str\n    timestamp: float\n\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n@dataclass\nclass TrackedDetection:\n    track_id: int\n    detection_type: str\n    bbox_history: List[BoundingBox] = field(default_factory=list)\n    captures: List[Capture] = field(default_factory=list)\n    is_confirmed: bool = False\n    \n    @property\n    def last_bbox(self) -> Optional[BoundingBox]:\n        return self.bbox_history[-1] if self.bbox_history else None\n\n    @property\n    def avg_confidence(self) -> float:\n        if not self.bbox_history: return 0.0\n        return sum(b.confidence for b in self.bbox_history) / len(self.bbox_history)\n    \n    @property\n    def best_capture(self) -> Optional[Capture]:\n        \"\"\"Retorna la captura con mayor confianza\"\"\"\n        if not self.captures:\n            return None\n        return max(self.captures, key=lambda c: c.bbox.confidence)\n    \n    def add_bbox(self, bbox: BoundingBox):\n        self.bbox_history.append(bbox)\n\n    # NUEVO: Método to_dict() que faltaba\n    def to_dict(self) -> dict:\n        return {\n            \"track_id\": self.track_id,\n            \"detection_type\": self.detection_type,\n            \"bbox_history\": [b.to_dict() for b in self.bbox_history],\n            \"captures\": [c.to_dict() for c in self.captures],\n            \"is_confirmed\": self.is_confirmed,\n            \"avg_confidence\": self.avg_confidence,\n            \"total_frames\": len(self.bbox_history)\n        }\n\n@dataclass\nclass DetectionResult:\n    video_path: str\n    total_frames: int\n    fps: float\n    duration_seconds: float\n    width: int\n    height: int\n    detections: List[TrackedDetection] = field(default_factory=list)\n    frames_processed: int = 0\n    processing_time_seconds: float = 0.0"
  },
  {
   "cell_type": "markdown",
   "id": "a389d0aa",
   "metadata": {},
   "source": [
    "### 3. Gestor de Capturas (`capture_manager.py`)\n",
    "Esta clase decide **cuándo** guardar una foto de un objeto.\n",
    "- No guarda todas las fotos de todos los frames (sería demasiado).\n",
    "- **`consider_frame`**: Evalúa si merece la pena capturar el objeto en el frame actual.\n",
    "    - Verifica **estabilidad**: Si la confianza de detección es alta durante varios frames seguidos.\n",
    "    - Verifica **tiempo**: Para no tener 50 fotos/segundo, impone un intervalo (ej. 1 foto cada segundo).\n",
    "- **`_save_capture`**: Recorta la imagen (`crop`) alrededor del objeto (con un margen extra) y la guarda en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54154cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureManager:\n",
    "    def __init__(self, stability_threshold=0.5, stability_frames=3, image_quality=95, crop_margin=20):\n",
    "        self.stability_threshold = stability_threshold\n",
    "        self.stability_frames = stability_frames\n",
    "        self.image_quality = image_quality\n",
    "        self.crop_margin = crop_margin\n",
    "        self.track_data = {} # {id: {stable_count: 0, last_capture_time: 0}}\n",
    "\n",
    "    def consider_frame(self, track_id: int, frame_img: np.ndarray, frame_num: int, bbox: BoundingBox, output_dir: Path, fps: float, capture_interval: float) -> str:\n",
    "        if track_id not in self.track_data:\n",
    "            self.track_data[track_id] = {\"stable_count\": 0, \"last_capture_time\": -999}\n",
    "            \n",
    "        data = self.track_data[track_id]\n",
    "        \n",
    "        # Check stability\n",
    "        if bbox.confidence >= self.stability_threshold:\n",
    "            data[\"stable_count\"] += 1\n",
    "        else:\n",
    "            data[\"stable_count\"] = 0\n",
    "            \n",
    "        if data[\"stable_count\"] < self.stability_frames:\n",
    "            return None\n",
    "            \n",
    "        # Check timing\n",
    "        timestamp = frame_num / fps\n",
    "        if timestamp - data[\"last_capture_time\"] < capture_interval:\n",
    "            return None\n",
    "            \n",
    "        return self._save_capture(track_id, frame_img, frame_num, bbox, output_dir, timestamp)\n",
    "\n",
    "    def _save_capture(self, track_id: int, frame_img: np.ndarray, frame_num: int, bbox: BoundingBox, output_dir: Path, timestamp: float) -> str:\n",
    "        track_dir = output_dir / f\"track_{track_id}\"\n",
    "        track_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        filename = f\"capture_{frame_num}.jpg\"\n",
    "        filepath = track_dir / filename\n",
    "        \n",
    "        h, w = frame_img.shape[:2]\n",
    "        x1 = max(0, int(bbox.x1) - self.crop_margin)\n",
    "        y1 = max(0, int(bbox.y1) - self.crop_margin)\n",
    "        x2 = min(w, int(bbox.x2) + self.crop_margin)\n",
    "        y2 = min(h, int(bbox.y2) + self.crop_margin)\n",
    "        \n",
    "        crop = frame_img[y1:y2, x1:x2]\n",
    "        if crop.size > 0:\n",
    "            cv2.imwrite(str(filepath), crop, [int(cv2.IMWRITE_JPEG_QUALITY), self.image_quality])\n",
    "            \n",
    "        self.track_data[track_id][\"last_capture_time\"] = timestamp\n",
    "        return str(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b98a85",
   "metadata": {},
   "source": [
    "### 4. Rastreador de Objetos (`tracker.py`)\n",
    "El modelo YOLO detecta objetos en *cada frame* de forma independiente. No sabe que la persona del frame 10 es la misma que la del frame 11.\n",
    "Esta clase soluciona eso:\n",
    "- **`Track`**: Representa un objeto vivo que estamos siguiendo.\n",
    "- **`ObjectTracker`**: Asigna IDs únicos a los objetos.\n",
    "- **`update`**: \n",
    "    1. Recibe las detecciones nuevas del frame actual.\n",
    "    2. Las compara con los `tracks` existentes usando **IOU** (Intersección sobre Unión). Si las cajas se solapan mucho, asume que es el mismo objeto.\n",
    "    3. Crea nuevos tracks para objetos que no coinciden con nada.\n",
    "    4. Elimina tracks antiguos que llevan mucho tiempo sin verse (`max_age`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba60be",
   "metadata": {},
   "outputs": [],
   "source": "class Track:\n    def __init__(self, track_id, detection_type, bbox, frame):\n        self.track_id = track_id\n        self.detection_type = detection_type\n        self.last_bbox = bbox\n        self.first_frame = frame\n        self.last_frame = frame\n        self.hits = 1\n        self.age = 0\n        \n    def update(self, bbox, frame):\n        self.last_bbox = bbox\n        self.last_frame = frame\n        self.hits += 1\n        self.age = 0\n\nclass ObjectTracker:\n    \"\"\"\n    Rastreador de objetos mejorado con Hungarian Algorithm para asignación óptima.\n    \"\"\"\n    def __init__(self, iou_threshold=0.3, max_age=30, min_hits=3):\n        self.iou_threshold = iou_threshold\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.tracks = {}\n        self.next_id = 1\n        \n    def update(self, detections: List[Tuple[str, BoundingBox]], frame_num: int) -> List[Tuple[int, str, BoundingBox]]:\n        # Envejecer tracks existentes\n        for t in self.tracks.values():\n            t.age += 1\n            \n        confirmed_out = []\n        dets_by_type = {}\n        for cls, bbox in detections:\n            dets_by_type.setdefault(cls, []).append(bbox)\n            \n        for cls, bboxes in dets_by_type.items():\n            active_tracks = [t for t in self.tracks.values() if t.detection_type == cls]\n            \n            # Si no hay tracks o detecciones, manejar casos edge\n            if not active_tracks:\n                for bbox in bboxes:\n                    self._create_track(cls, bbox, frame_num)\n                continue\n                \n            if not bboxes:\n                continue\n            \n            # MEJORADO: Construir matriz de costos para Hungarian Algorithm\n            num_tracks = len(active_tracks)\n            num_dets = len(bboxes)\n            cost_matrix = np.ones((num_tracks, num_dets), dtype=np.float32)\n            \n            for i, trk in enumerate(active_tracks):\n                for j, det in enumerate(bboxes):\n                    iou = self._calculate_iou(trk.last_bbox, det)\n                    if iou >= self.iou_threshold:\n                        cost_matrix[i, j] = 1.0 - iou  # Menor costo = mejor match\n            \n            # MEJORADO: Asignación óptima con Hungarian Algorithm (scipy)\n            row_indices, col_indices = linear_sum_assignment(cost_matrix)\n            \n            matched_tracks = set()\n            matched_dets = set()\n            \n            for row, col in zip(row_indices, col_indices):\n                # Solo aceptar si el costo es aceptable (IOU >= threshold)\n                if cost_matrix[row, col] < (1.0 - self.iou_threshold):\n                    track = active_tracks[row]\n                    matched_tracks.add(track.track_id)\n                    matched_dets.add(col)\n                    self.tracks[track.track_id].update(bboxes[col], frame_num)\n                    \n            # Crear nuevos tracks para detecciones sin match\n            for j, det in enumerate(bboxes):\n                if j not in matched_dets:\n                    self._create_track(cls, det, frame_num)\n                    \n        # Limpieza de tracks muertos y reporte de confirmados\n        dead = []\n        for tid, t in self.tracks.items():\n            if t.age > self.max_age:\n                dead.append(tid)\n            elif t.hits >= self.min_hits:\n                confirmed_out.append((tid, t.detection_type, t.last_bbox))\n                \n        for tid in dead:\n            del self.tracks[tid]\n        \n        return confirmed_out\n\n    def _create_track(self, cls, bbox, frame):\n        self.tracks[self.next_id] = Track(self.next_id, cls, bbox, frame)\n        self.next_id += 1\n\n    def _calculate_iou(self, bb1, bb2):\n        \"\"\"Calcula Intersection over Union entre dos bounding boxes\"\"\"\n        xl = max(bb1.x1, bb2.x1)\n        yt = max(bb1.y1, bb2.y1)\n        xr = min(bb1.x2, bb2.x2)\n        yb = min(bb1.y2, bb2.y2)\n        if xr < xl or yb < yt:\n            return 0.0\n        inter = (xr - xl) * (yb - yt)\n        union = bb1.area + bb2.area - inter\n        return inter / union if union > 0 else 0.0"
  },
  {
   "cell_type": "markdown",
   "id": "76ab48fc",
   "metadata": {},
   "source": "### 5. Detector Principal (`detector.py`) - Arquitectura Híbrida\n\nEsta es la clase maestra que orquesta todo el proceso usando **HybridDetectorManager**:\n\n**Arquitectura de Detección:**\n| Tipo | Motor | Modelo |\n|------|-------|--------|\n| Caras | Kornia AI | YuNet (FaceDetector) |\n| Personas | Ultralytics | YOLOv10 (nano/s/m) |\n| Matrículas | Ultralytics | YOLO-LPR |\n\n**Flujo de Procesamiento:**\n1. `__init__`: Inicializa `HybridDetectorManager` que carga:\n   - Kornia FaceDetector (YuNet) para caras - nativo GPU\n   - YOLOv10 para personas - selección automática según VRAM\n   - YOLO-LPR para matrículas (opcional)\n2. `process_video`: \n   - Abre el video frame a frame con OpenCV\n   - Acumula frames en batches según `batch_size` (auto-calculado)\n   - Ejecuta detección híbrida: Kornia (caras) + YOLO (personas/matrículas)\n   - Pasa detecciones al `tracker` para IDs estables\n   - Guarda capturas con `CaptureManager`\n3. `_process_batch`: Procesa un lote de frames en paralelo/secuencial según VRAM\n\n**Auto-Adaptación por VRAM:**\n| VRAM | Estrategia | Modelos | Batch |\n|------|------------|---------|-------|\n| <8GB | Secuencial | nano | 4 |\n| 8-16GB | Paralelo | small | 16 |\n| 16GB+ | Paralelo | medium | 32-64 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70c017",
   "metadata": {},
   "outputs": [],
   "source": "class VideoDetector:\n    \"\"\"\n    Detector multi-modelo para datos sensibles GDPR.\n\n    Utiliza HybridDetectorManager (Kornia FaceDetector + YOLOv10) para gestionar\n    automáticamente los modelos según la VRAM disponible.\n\n    Arquitectura Híbrida:\n    - Personas: YOLOv10 (nano/s/m según VRAM)\n    - Caras: Kornia YuNet (FaceDetector) - nativo GPU, sin dependencias extra\n    - Matrículas: YOLO-LPR\n    - Huellas/Documentos: requiere módulo de verificación con LLM\n    \"\"\"\n\n    def __init__(\n        self,\n        hybrid_manager: HybridDetectorManager = None,\n        person_model: str = None,\n        plate_model: str = None,\n        confidence_threshold: float = 0.5\n    ):\n        \"\"\"\n        Args:\n            hybrid_manager: HybridDetectorManager preconfigurado (opcional)\n            person_model: Ruta personalizada al modelo de personas (YOLOv10)\n            plate_model: Ruta personalizada al modelo de matrículas\n            confidence_threshold: Umbral de confianza para detecciones\n        \"\"\"\n        self.conf_threshold = confidence_threshold\n\n        # Usar manager proporcionado o crear uno nuevo\n        if hybrid_manager:\n            self.hybrid_manager = hybrid_manager\n        else:\n            self.hybrid_manager = HybridDetectorManager(\n                person_model=person_model,\n                plate_model=plate_model,\n                face_confidence=confidence_threshold,\n                person_confidence=confidence_threshold\n            )\n\n        self.batch_size = self.hybrid_manager.batch_size\n        self.device = self.hybrid_manager.device\n\n        logger.info(f\"VideoDetector initialized: {self.hybrid_manager.get_info()}\")\n\n    async def process_video(\n        self, \n        video_path: str, \n        output_dir: str,\n        on_progress: Optional[Callable[[int, int, str], None]] = None\n    ) -> 'DetectionResult':\n        \"\"\"\n        Procesa un video detectando datos sensibles GDPR.\n        \n        Args:\n            video_path: Ruta al archivo de video\n            output_dir: Directorio para guardar capturas\n            on_progress: Callback de progreso (current, total, message)\n            \n        Returns:\n            DetectionResult con todas las detecciones encontradas\n        \"\"\"\n        import time\n        start_time = time.time()\n        \n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Could not open video: {video_path}\")\n            \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        tracker = ObjectTracker()\n        cm = CaptureManager()\n        tracked_objects = {}\n        frame_num = 0\n        \n        frame_buffer = []\n        frame_nums = []\n        \n        logger.info(f\"Processing video: {total_frames} frames at {fps} FPS ({width}x{height})\")\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                if frame_buffer:\n                    await self._process_batch(\n                        frame_buffer, frame_nums, tracker, cm,\n                        tracked_objects, output_path, fps, on_progress, total_frames\n                    )\n                break\n            \n            frame_num += 1\n            frame_buffer.append(frame)\n            frame_nums.append(frame_num)\n            \n            if len(frame_buffer) >= self.batch_size:\n                await self._process_batch(\n                    frame_buffer, frame_nums, tracker, cm,\n                    tracked_objects, output_path, fps, on_progress, total_frames\n                )\n                frame_buffer = []\n                frame_nums = []\n                \n        cap.release()\n        \n        processing_time = time.time() - start_time\n        \n        return DetectionResult(\n            video_path=video_path, \n            total_frames=total_frames, \n            fps=fps, \n            duration_seconds=total_frames/fps if fps > 0 else 0, \n            width=width, \n            height=height, \n            detections=list(tracked_objects.values()),\n            frames_processed=frame_num,\n            processing_time_seconds=processing_time\n        )\n\n    async def _process_batch(\n        self, \n        frames: List[np.ndarray], \n        frame_nums: List[int], \n        tracker: ObjectTracker, \n        cm: CaptureManager,\n        tracked_objects: Dict[int, 'TrackedDetection'], \n        output_path: Path, \n        fps: float,\n        on_progress: Optional[Callable],\n        total_frames: int\n    ):\n        \"\"\"Procesa un batch de frames usando HybridDetectorManager\"\"\"\n        \n        # Obtener detecciones de todos los modelos usando HybridDetectorManager\n        all_detections_by_frame = await self.hybrid_manager.detect_all(\n            frames, \n            frame_nums, \n            conf_threshold=self.conf_threshold\n        )\n        \n        # Procesar cada frame\n        for frame, frame_num in zip(frames, frame_nums):\n            current_detections = all_detections_by_frame[frame_num]\n            \n            # Actualizar Tracker\n            confirmed = tracker.update(current_detections, frame_num)\n            \n            for tid, label, bbox in confirmed:\n                if tid not in tracked_objects:\n                    tracked_objects[tid] = TrackedDetection(tid, label)\n                tracked_objects[tid].add_bbox(bbox)\n                \n                cap_path = cm.consider_frame(\n                    tid, frame, frame_num, bbox, output_path, fps, \n                    capture_interval=1.0\n                )\n                if cap_path:\n                    tracked_objects[tid].captures.append(Capture(\n                        frame=frame_num, \n                        image_path=cap_path, \n                        bbox=bbox, \n                        reason=\"periodic\", \n                        timestamp=frame_num/fps\n                    ))\n            \n            if frame_num % 10 == 0 and on_progress:\n                msg = f\"Frame {frame_num}/{total_frames}\"\n                if asyncio.iscoroutinefunction(on_progress):\n                    await on_progress(frame_num, total_frames, msg)\n                else:\n                    on_progress(frame_num, total_frames, msg)\n        \n        await asyncio.sleep(0)  # Yield para otras tareas async"
  },
  {
   "cell_type": "markdown",
   "id": "5cf4990b",
   "metadata": {},
   "source": [
    "### 6. Ejecución de Prueba\n",
    "Bloque final para verificar que todo funciona:\n",
    "1. Define las rutas de entrada (video) y salida.\n",
    "2. Instancia el `VideoDetector`.\n",
    "3. Ejecuta `process_video` en un bucle asíncrono (`asyncio.run`).\n",
    "4. Imprime un resumen de los objetos encontrados y sus capturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PRUEBA DE EJECUCIÓN - ARQUITECTURA HÍBRIDA (Kornia + YOLOv10)\n# =============================================================================\n\nVIDEO_PATH = \"../storage/uploads/coche.mp4\"  # Actualiza esto\nOUTPUT_DIR = \"../storage/captures/notebook_test_hybrid\"\n\n# Rutas de modelos personalizados (opcional)\n# El sistema auto-selecciona nano/small/medium según tu VRAM\nCUSTOM_PERSON_MODEL = None  # e.g., \"../models/yolov10m.pt\"\nCUSTOM_PLATE_MODEL = None   # e.g., \"../models/yolo-license-plate.pt\"\n\nasync def run_hybrid_test():\n    if not os.path.exists(VIDEO_PATH):\n        print(f\"Video no encontrado: {VIDEO_PATH}\")\n        print(\"Saltando test. Para ejecutar, descarga un video de prueba.\")\n        return\n    \n    print(\"=\" * 70)\n    print(\"PRUEBA DEL MÓDULO DE DETECCIÓN - ARQUITECTURA HÍBRIDA\")\n    print(\"Kornia FaceDetector (YuNet) + YOLOv10 (Personas) + YOLO-LPR (Matrículas)\")\n    print(\"=\" * 70)\n    \n    # El detector usa HybridDetectorManager internamente\n    # Se adaptará automáticamente a tu GPU (nano/small/medium)\n    detector = VideoDetector(\n        person_model=CUSTOM_PERSON_MODEL,\n        plate_model=CUSTOM_PLATE_MODEL,\n        confidence_threshold=0.5\n    )\n    \n    # Mostrar configuración del manager híbrido\n    info = detector.hybrid_manager.get_info()\n    print(f\"\\nConfiguración Híbrida Auto-Adaptativa:\")\n    print(f\"  - Estrategia: {info['strategy']}\")\n    print(f\"  - Tamaño de modelos YOLO: {info['model_size']}\")\n    print(f\"  - Batch size: {info['batch_size']}\")\n    print(f\"  - Dispositivo: {info['device']}\")\n    print(f\"  - VRAM: {info['vram_total_mb']}MB\")\n    print(f\"  - Kornia FaceDetector: {'✓' if info['kornia_available'] else '✗ (usando OpenCV Haar)'}\")\n    print(f\"  - Detectores activos: {', '.join(info['detectors'])}\")\n    \n    print(f\"\\nProcesando video: {VIDEO_PATH}\")\n    print(\"-\" * 70)\n    \n    def progress_callback(current, total, msg):\n        pct = (current / total) * 100\n        print(f\"\\r  Progreso: {pct:.1f}% ({msg})\", end=\"\", flush=True)\n    \n    res = await detector.process_video(\n        VIDEO_PATH, \n        OUTPUT_DIR,\n        on_progress=progress_callback\n    )\n    \n    print(f\"\\n\\n{'=' * 70}\")\n    print(\"RESULTADOS\")\n    print(\"=\" * 70)\n    print(f\"  - Frames procesados: {res.frames_processed}/{res.total_frames}\")\n    print(f\"  - Tiempo de procesamiento: {res.processing_time_seconds:.2f}s\")\n    print(f\"  - FPS de procesamiento: {res.frames_processed / res.processing_time_seconds:.1f}\")\n    print(f\"  - Objetos detectados: {len(res.detections)}\")\n    \n    # Agrupar por tipo\n    by_type = {}\n    for det in res.detections:\n        by_type.setdefault(det.detection_type, []).append(det)\n    \n    print(\"\\nResumen por tipo de detección (Arquitectura Híbrida):\")\n    for det_type, dets in by_type.items():\n        severity = GDPR_SEVERITY.get(det_type, \"unknown\")\n        total_captures = sum(len(d.captures) for d in dets)\n        engine = \"Kornia YuNet\" if det_type == \"face\" else \"YOLOv10\" if det_type == \"person\" else \"YOLO-LPR\"\n        print(f\"  - {det_type}: {len(dets)} objetos, {total_captures} capturas \"\n              f\"(Motor: {engine}, Severidad GDPR: {severity})\")\n    \n    print(\"\\nPrimeras 5 detecciones:\")\n    for det in res.detections[:5]:\n        best = det.best_capture\n        best_frame = best.frame if best else \"N/A\"\n        print(f\"  ID: {det.track_id}, Tipo: {det.detection_type}, \"\n              f\"Frames: {len(det.bbox_history)}, Capturas: {len(det.captures)}, \"\n              f\"Mejor frame: {best_frame}\")\n    \n    print(f\"\\nCapturas guardadas en: {OUTPUT_DIR}\")\n\n# Ejecutar test\nasyncio.run(run_hybrid_test())"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}