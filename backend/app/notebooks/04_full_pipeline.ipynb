{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Pipeline Completo - OccultaShield\n",
    "\n",
    "Este notebook integra todos los módulos optimizados para crear un pipeline completo de anonimización GDPR:\n",
    "\n",
    "**Arquitectura Híbrida (Kornia + YOLO):**\n",
    "1. **Detección de Caras**: Kornia FaceDetector (YuNet) - nativo GPU\n",
    "2. **Detección de Personas**: YOLOv10 (nano/s/m según VRAM)\n",
    "3. **Detección de Matrículas**: YOLO-LPR\n",
    "4. **Edición**: KorniaEffects para efectos acelerados por GPU\n",
    "5. **Verificación**: GemmaClient con clasificación LLM para huellas/documentos\n",
    "\n",
    "**Flujo:**\n",
    "```\n",
    "Video → Detección Híbrida → Verificación GDPR → Anonimización GPU → Video Procesado\n",
    "        (Kornia+YOLO)        (LLM + Graph)       (Kornia filters)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "Importamos los componentes de los otros módulos (asumiendo que los archivos .py existen y son estables). Si usasemos solo los notebooks, copiaríamos el código aquí, pero importar mantiene este notebook limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport asyncio\nimport time\nimport logging\nimport nest_asyncio\nimport json\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Any, Tuple\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# Kornia para efectos GPU y detección de caras (YuNet)\ntry:\n    import kornia\n    import kornia.filters\n    from kornia.contrib import FaceDetector, FaceDetectorResult\n    KORNIA_AVAILABLE = True\n    KORNIA_FACE_AVAILABLE = True\nexcept ImportError:\n    try:\n        import kornia\n        import kornia.filters\n        KORNIA_AVAILABLE = True\n        KORNIA_FACE_AVAILABLE = False\n    except ImportError:\n        KORNIA_AVAILABLE = False\n        KORNIA_FACE_AVAILABLE = False\n\nfrom ultralytics import YOLO\nfrom scipy.optimize import linear_sum_assignment\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Módulo de verificación GDPR (Neo4j + Gemma 3n)\ntry:\n    sys.path.insert(0, str(Path(\"..\").resolve()))\n    from modules.verification import verify_image_detections\n    VERIFICATION_MODULE_AVAILABLE = True\nexcept ImportError as e:\n    VERIFICATION_MODULE_AVAILABLE = False\n    print(f\"[!] Módulo de verificación no disponible: {e}\")\n    print(\"    Se usará verificación con GDPR_CONFIG hardcodeado\")\n\nnest_asyncio.apply()\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('full_pipeline')\n\nprint(\"=\" * 70)\nprint(\"OCCULTASHIELD - Pipeline de Anonimización GDPR (Arquitectura Híbrida)\")\nprint(\"=\" * 70)\nprint(f\"✓ PyTorch: {torch.__version__}\")\nprint(f\"✓ CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n    vram_mb = torch.cuda.get_device_properties(0).total_memory // (1024**2)\n    print(f\"✓ VRAM: {vram_mb}MB\")\nprint(f\"✓ Kornia disponible: {KORNIA_AVAILABLE}\")\nprint(f\"✓ Kornia FaceDetector (YuNet): {KORNIA_FACE_AVAILABLE}\")\nprint(f\"✓ Módulo de Verificación (Neo4j + Gemma 3n): {VERIFICATION_MODULE_AVAILABLE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mocks de Infraestructura\n",
    "Como corremos en local sin base de datos real (SurrealDB) ni backend web:\n",
    "- **`MockProgressManager`**: Imprime por consola en vez de enviar WebSockets al frontend.\n",
    "- **`MockDB`**: Simula guardar datos devolviendo un ID falso, sin conectar a nada.\n",
    "- **`ProcessingPhase`**: Constantes para saber en qué etapa estamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACIÓN GDPR Y MOCKS\n",
    "# =============================================================================\n",
    "\n",
    "# Mapeo de severidad y acciones GDPR\n",
    "GDPR_CONFIG = {\n",
    "    \"face\": {\"severity\": \"high\", \"action\": \"blur\", \"articles\": [\"9\", \"6\"]},\n",
    "    \"fingerprint\": {\"severity\": \"high\", \"action\": \"pixelate\", \"articles\": [\"9\", \"6\"]},\n",
    "    \"license_plate\": {\"severity\": \"high\", \"action\": \"pixelate\", \"articles\": [\"6\", \"17\"]},\n",
    "    \"id_document\": {\"severity\": \"high\", \"action\": \"blur\", \"articles\": [\"9\", \"6\", \"32\"]},\n",
    "    \"credit_card\": {\"severity\": \"high\", \"action\": \"pixelate\", \"articles\": [\"6\", \"32\"]},\n",
    "    \"signature\": {\"severity\": \"medium\", \"action\": \"blur\", \"articles\": [\"6\"]},\n",
    "    \"person\": {\"severity\": \"medium\", \"action\": \"blur\", \"articles\": [\"6\", \"13\"]},\n",
    "}\n",
    "\n",
    "class MockProgressManager:\n",
    "    \"\"\"Simula el gestor de progreso WebSocket del backend\"\"\"\n",
    "    async def register_video(self, video_id): \n",
    "        print(f\"\\n[PIPELINE] Iniciado: {video_id}\")\n",
    "    \n",
    "    async def change_phase(self, video_id, phase, msg, **kwargs): \n",
    "        print(f\"[FASE] {phase}: {msg}\")\n",
    "    \n",
    "    async def update_progress(self, video_id, pct, cur, tot, msg): \n",
    "        if cur % 30 == 0 or pct >= 100:\n",
    "            print(f\"  └─ Progreso: {pct:3d}% | {msg}\")\n",
    "    \n",
    "    async def complete(self, video_id, **kwargs): \n",
    "        print(f\"\\n[✓] COMPLETADO\")\n",
    "        for k, v in kwargs.items():\n",
    "            print(f\"    {k}: {v}\")\n",
    "    \n",
    "    async def error(self, video_id, code, msg, **kwargs): \n",
    "        print(f\"\\n[✗] ERROR: {msg}\")\n",
    "\n",
    "class MockDB:\n",
    "    \"\"\"Simula la base de datos\"\"\"\n",
    "    async def create(self, table, data):\n",
    "        return [{\"id\": f\"{table}:mock_{id(data)}\", **data}]\n",
    "\n",
    "class ProcessingPhase:\n",
    "    DETECTING = \"detecting\"\n",
    "    TRACKING = \"tracking\"\n",
    "    VERIFYING = \"verifying\"\n",
    "    EDITING = \"editing\"\n",
    "    COMPLETED = \"completed\"\n",
    "\n",
    "# =============================================================================\n",
    "# MODELOS DE DATOS (copiados de 01_detection_module para independencia)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    x1: float\n",
    "    y1: float\n",
    "    x2: float\n",
    "    y2: float\n",
    "    confidence: float\n",
    "    frame: int\n",
    "\n",
    "    @property\n",
    "    def area(self) -> float:\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class Capture:\n",
    "    frame: int\n",
    "    image_path: str\n",
    "    bbox: BoundingBox\n",
    "    reason: str\n",
    "    timestamp: float\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class TrackedDetection:\n",
    "    track_id: int\n",
    "    detection_type: str\n",
    "    bbox_history: List[BoundingBox] = field(default_factory=list)\n",
    "    captures: List[Capture] = field(default_factory=list)\n",
    "    is_confirmed: bool = False\n",
    "    \n",
    "    @property\n",
    "    def best_capture(self) -> Optional[Capture]:\n",
    "        if not self.captures:\n",
    "            return None\n",
    "        return max(self.captures, key=lambda c: c.bbox.confidence)\n",
    "    \n",
    "    def add_bbox(self, bbox: BoundingBox):\n",
    "        self.bbox_history.append(bbox)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"track_id\": self.track_id,\n",
    "            \"detection_type\": self.detection_type,\n",
    "            \"bbox_history\": [b.to_dict() for b in self.bbox_history],\n",
    "            \"captures\": [c.to_dict() for c in self.captures],\n",
    "            \"is_confirmed\": self.is_confirmed,\n",
    "            \"total_frames\": len(self.bbox_history)\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    video_path: str\n",
    "    total_frames: int\n",
    "    fps: float\n",
    "    duration_seconds: float\n",
    "    width: int\n",
    "    height: int\n",
    "    detections: List[TrackedDetection] = field(default_factory=list)\n",
    "    frames_processed: int = 0\n",
    "    processing_time_seconds: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Orquestador (`VideoProcessor`) - Arquitectura Híbrida\n",
    "\n",
    "Esta clase une todo el flujo usando **HybridDetectorManager** (Kornia + YOLO).\n",
    "\n",
    "**Componentes:**\n",
    "- **HybridDetectorManager**: Kornia FaceDetector (YuNet) + YOLOv10 para personas + YOLO-LPR para matrículas\n",
    "- **KorniaEffects**: Blur/Pixelate GPU acelerado\n",
    "- **GDPRConfig**: Acciones por tipo de detección\n",
    "\n",
    "**Flujo `process_full_pipeline`:**\n",
    "1. **Detectar**: Usa `HybridDetectorManager.detect_all()`:\n",
    "   - Caras → Kornia FaceDetector (YuNet)\n",
    "   - Personas → YOLOv10 (nano/s/m según VRAM)\n",
    "   - Matrículas → YOLO-LPR\n",
    "2. **Tracking**: Asigna IDs estables con `ObjectTracker`\n",
    "3. **Verificar GDPR**: Clasifica violaciones según severidad\n",
    "4. **Anonimizar**: Aplica efectos GPU con `KorniaEffects` (blur/pixelate)\n",
    "5. **Completar**: Reporta éxito con métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# GPU MANAGER\n# =============================================================================\nclass GPUManager:\n    _instance = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialized = False\n        return cls._instance\n    \n    def __init__(self):\n        if self._initialized:\n            return\n        self._initialized = True\n        \n        if torch.cuda.is_available():\n            self.device = \"cuda\"\n            self.device_name = torch.cuda.get_device_name(0)\n            self.vram_total_mb = torch.cuda.get_device_properties(0).total_memory // (1024**2)\n        else:\n            self.device = \"cpu\"\n            self.device_name = \"CPU\"\n            self.vram_total_mb = 0\n    \n    def get_strategy(self) -> Tuple[str, str, int]:\n        vram_gb = self.vram_total_mb / 1024\n        if vram_gb < 8:\n            return \"sequential\", \"nano\", 4\n        elif vram_gb < 16:\n            return \"parallel\", \"small\", 16\n        else:\n            return \"parallel\", \"medium\", min(64, int(vram_gb * 2))\n\ngpu_manager = GPUManager()\n\n# =============================================================================\n# HYBRID DETECTOR MANAGER - Kornia FaceDetector + YOLOv10\n# =============================================================================\nclass HybridDetectorManager:\n    \"\"\"\n    Gestor híbrido de detectores: Kornia AI (caras) + YOLO (personas, matrículas).\n    \n    Arquitectura:\n    - Caras: Kornia YuNet (FaceDetector) - nativo GPU\n    - Personas: YOLOv10 (nano/s/m según VRAM)\n    - Matrículas: YOLO-LPR\n    \"\"\"\n    \n    YOLO_CONFIGS = {\n        \"nano\": {\"person\": \"yolov10n.pt\", \"plate\": \"yolov8n.pt\"},\n        \"small\": {\"person\": \"yolov10s.pt\", \"plate\": \"yolov8s.pt\"},\n        \"medium\": {\"person\": \"yolov10m.pt\", \"plate\": \"yolov8m.pt\"},\n    }\n    \n    def __init__(\n        self, \n        gpu_mgr: GPUManager = None,\n        person_model: str = None,\n        plate_model: str = None,\n        face_confidence: float = 0.5,\n        person_confidence: float = 0.5\n    ):\n        self.gpu = gpu_mgr or gpu_manager\n        self.device = self.gpu.device\n        self.strategy, self.model_size, self.batch_size = self.gpu.get_strategy()\n        \n        self.face_confidence = face_confidence\n        self.person_confidence = person_confidence\n        \n        self._init_face_detector()\n        self._init_yolo_detectors(person_model, plate_model)\n        \n        logger.info(f\"HybridDetectorManager: strategy={self.strategy}, size={self.model_size}, \"\n                   f\"device={self.device}, kornia_face={KORNIA_FACE_AVAILABLE}\")\n    \n    def _init_face_detector(self):\n        \"\"\"Inicializa Kornia FaceDetector (YuNet)\"\"\"\n        self.face_detector = None\n        \n        if KORNIA_FACE_AVAILABLE:\n            try:\n                self.face_detector = FaceDetector().to(self.device)\n                logger.info(\"✓ Kornia FaceDetector (YuNet) loaded\")\n            except Exception as e:\n                logger.warning(f\"Could not load Kornia FaceDetector: {e}\")\n        \n        # Fallback OpenCV Haar\n        if self.face_detector is None:\n            self.face_cascade = cv2.CascadeClassifier(\n                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n            )\n            logger.info(\"Using OpenCV Haar Cascade fallback for face detection\")\n    \n    def _init_yolo_detectors(self, person_model: str, plate_model: str):\n        \"\"\"Inicializa detectores YOLO para personas y matrículas\"\"\"\n        config = self.YOLO_CONFIGS[self.model_size]\n        \n        # Detector de personas (YOLOv10)\n        person_path = person_model or config[\"person\"]\n        try:\n            self.person_detector = YOLO(person_path)\n            logger.info(f\"✓ YOLO person detector loaded: {person_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to load person model: {e}\")\n            self.person_detector = None\n        \n        # Detector de matrículas (opcional)\n        self.plate_detector = None\n        if plate_model and os.path.exists(plate_model):\n            try:\n                self.plate_detector = YOLO(plate_model)\n                logger.info(f\"✓ YOLO plate detector loaded: {plate_model}\")\n            except Exception as e:\n                logger.warning(f\"Could not load plate model: {e}\")\n    \n    def _numpy_to_tensor(self, frame: np.ndarray) -> torch.Tensor:\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        tensor = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n        return tensor.to(self.device)\n    \n    def detect_faces_kornia(self, tensor: torch.Tensor, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n        \"\"\"\n        Detecta caras usando Kornia FaceDetector (YuNet).\n        Usa FaceDetectorResult para decodificar correctamente las detecciones.\n        \"\"\"\n        if self.face_detector is None:\n            return []\n        \n        results = []\n        \n        with torch.no_grad():\n            detections = self.face_detector(tensor)\n        \n        # Decodificar cada detección usando FaceDetectorResult\n        for det in detections:\n            try:\n                # Usar FaceDetectorResult para parsear correctamente\n                face_result = FaceDetectorResult(det)\n                \n                # Verificar si hay detecciones\n                if face_result.score.numel() == 0:\n                    continue\n                \n                # Obtener coordenadas y scores\n                top_left = face_result.top_left.int().tolist()\n                bottom_right = face_result.bottom_right.int().tolist()\n                scores = face_result.score.tolist()\n                \n                # Procesar cada cara detectada\n                for score, tl, br in zip(scores, top_left, bottom_right):\n                    if score >= self.face_confidence:\n                        x1, y1 = float(tl[0]), float(tl[1])\n                        x2, y2 = float(br[0]), float(br[1])\n                        \n                        bbox = BoundingBox(x1, y1, x2, y2, float(score), frame_num)\n                        \n                        if bbox.area >= 500:  # MIN_DETECTION_AREA\n                            results.append((\"face\", bbox))\n                            \n            except Exception as e:\n                logger.debug(f\"Error processing face detection: {e}\")\n        \n        return results\n    \n    def detect_faces_opencv(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n        \"\"\"Fallback: Detección de caras con OpenCV Haar Cascade\"\"\"\n        if not hasattr(self, 'face_cascade'):\n            return []\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n        \n        results = []\n        for (x, y, w, h) in faces:\n            bbox = BoundingBox(float(x), float(y), float(x + w), float(y + h), 0.8, frame_num)\n            if bbox.area >= 500:\n                results.append((\"face\", bbox))\n        return results\n    \n    def detect_persons(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n        \"\"\"Detecta personas usando YOLOv10\"\"\"\n        if self.person_detector is None:\n            return []\n        \n        results_yolo = self.person_detector.predict(\n            frame, conf=self.person_confidence, verbose=False, device=self.device\n        )\n        \n        results = []\n        for r in results_yolo:\n            for box in r.boxes:\n                cls = int(box.cls[0])\n                if cls == 0:  # COCO class 0 = person\n                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n                    bbox = BoundingBox(x1, y1, x2, y2, float(box.conf[0]), frame_num)\n                    if bbox.area >= 500:\n                        results.append((\"person\", bbox))\n        return results\n    \n    def detect_plates(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n        \"\"\"Detecta matrículas usando YOLO-LPR\"\"\"\n        if self.plate_detector is None:\n            return []\n        \n        results_yolo = self.plate_detector.predict(\n            frame, conf=self.person_confidence, verbose=False, device=self.device\n        )\n        \n        results = []\n        for r in results_yolo:\n            for box in r.boxes:\n                x1, y1, x2, y2 = box.xyxy[0].tolist()\n                bbox = BoundingBox(x1, y1, x2, y2, float(box.conf[0]), frame_num)\n                if bbox.area >= 100:\n                    results.append((\"license_plate\", bbox))\n        return results\n    \n    def detect_all(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n        \"\"\"Ejecuta todos los detectores en un frame\"\"\"\n        all_detections = []\n        \n        # Personas (YOLOv10)\n        all_detections.extend(self.detect_persons(frame, frame_num))\n        \n        # Caras (Kornia o OpenCV fallback)\n        if self.face_detector is not None:\n            tensor = self._numpy_to_tensor(frame)\n            all_detections.extend(self.detect_faces_kornia(tensor, frame_num))\n        else:\n            all_detections.extend(self.detect_faces_opencv(frame, frame_num))\n        \n        # Matrículas (YOLO)\n        all_detections.extend(self.detect_plates(frame, frame_num))\n        \n        return all_detections\n    \n    def get_info(self) -> Dict:\n        detectors = []\n        if self.person_detector:\n            detectors.append(\"person (YOLOv10)\")\n        if self.face_detector:\n            detectors.append(\"face (Kornia YuNet)\")\n        elif hasattr(self, 'face_cascade'):\n            detectors.append(\"face (OpenCV Haar)\")\n        if self.plate_detector:\n            detectors.append(\"plate (YOLO)\")\n        \n        return {\n            \"strategy\": self.strategy,\n            \"model_size\": self.model_size,\n            \"batch_size\": self.batch_size,\n            \"device\": self.device,\n            \"vram_total_mb\": self.gpu.vram_total_mb,\n            \"detectors\": detectors,\n            \"kornia_available\": KORNIA_FACE_AVAILABLE\n        }\n\n# =============================================================================\n# KORNIA EFFECTS (de 02_edition_module)\n# =============================================================================\nclass KorniaEffects:\n    def __init__(self, device: str = None):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.noise_cache = {}\n    \n    def numpy_to_tensor(self, frame: np.ndarray) -> torch.Tensor:\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        tensor = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0)\n        return tensor.float().div(255.0).to(self.device)\n    \n    def tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:\n        arr = tensor.squeeze(0).permute(1, 2, 0)\n        arr = arr.mul(255.0).clamp(0, 255).byte().cpu().numpy()\n        return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n    \n    def blur_region(self, tensor, bbox, kernel_size=31, sigma=15.0):\n        x1, y1, x2, y2 = bbox\n        result = tensor.clone()\n        roi = tensor[:, :, y1:y2, x1:x2]\n        \n        if KORNIA_AVAILABLE and roi.numel() > 0:\n            ks = kernel_size | 1\n            blurred = kornia.filters.gaussian_blur2d(roi, (ks, ks), (sigma, sigma))\n            result[:, :, y1:y2, x1:x2] = blurred\n        \n        return result\n    \n    def pixelate_region(self, tensor, bbox, blocks=10, track_id=0):\n        x1, y1, x2, y2 = bbox\n        result = tensor.clone()\n        roi = tensor[:, :, y1:y2, x1:x2]\n        \n        if roi.shape[2] < 2 or roi.shape[3] < 2:\n            return result\n        \n        small = F.interpolate(roi, size=(blocks, blocks), mode='bilinear', align_corners=False)\n        \n        cache_key = (track_id, blocks)\n        if cache_key not in self.noise_cache:\n            gen = torch.Generator(device=self.device).manual_seed(track_id * 1000)\n            self.noise_cache[cache_key] = torch.rand(1, 3, blocks, blocks, generator=gen, device=self.device) * 0.2 - 0.1\n        \n        small = (small + self.noise_cache[cache_key]).clamp(0, 1)\n        pixelated = F.interpolate(small, size=(y2-y1, x2-x1), mode='nearest')\n        result[:, :, y1:y2, x1:x2] = pixelated\n        \n        return result\n    \n    def clear_cache(self):\n        self.noise_cache.clear()\n\nkornia_effects = KorniaEffects() if KORNIA_AVAILABLE else None\n\n# =============================================================================\n# TRACKER SIMPLIFICADO\n# =============================================================================\nclass ObjectTracker:\n    def __init__(self, iou_threshold=0.3, max_age=30, min_hits=3):\n        self.iou_threshold = iou_threshold\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.tracks = {}\n        self.next_id = 1\n    \n    def update(self, detections, frame_num):\n        for t in self.tracks.values():\n            t['age'] += 1\n        \n        confirmed = []\n        for cls, bbox in detections:\n            matched = False\n            for tid, t in self.tracks.items():\n                if t['cls'] == cls and self._iou(t['bbox'], bbox) >= self.iou_threshold:\n                    t['bbox'] = bbox\n                    t['age'] = 0\n                    t['hits'] += 1\n                    matched = True\n                    break\n            \n            if not matched:\n                self.tracks[self.next_id] = {'cls': cls, 'bbox': bbox, 'age': 0, 'hits': 1}\n                self.next_id += 1\n        \n        dead = [tid for tid, t in self.tracks.items() if t['age'] > self.max_age]\n        for tid in dead:\n            del self.tracks[tid]\n        \n        for tid, t in self.tracks.items():\n            if t['hits'] >= self.min_hits:\n                confirmed.append((tid, t['cls'], t['bbox']))\n        \n        return confirmed\n    \n    def _iou(self, b1, b2):\n        x1 = max(b1.x1, b2.x1)\n        y1 = max(b1.y1, b2.y1)\n        x2 = min(b1.x2, b2.x2)\n        y2 = min(b1.y2, b2.y2)\n        if x2 < x1 or y2 < y1:\n            return 0\n        inter = (x2 - x1) * (y2 - y1)\n        union = b1.area + b2.area - inter\n        return inter / union if union > 0 else 0\n\n# =============================================================================\n# GENERADOR DE JSON DE SALIDA\n# =============================================================================\ndef generate_output_json(\n    video_id: str,\n    detection_result: DetectionResult,\n    violations: List[Dict],\n    output_path: Path,\n    processing_time: float\n) -> Dict[str, Any]:\n    \"\"\"\n    Genera el JSON de resultados del pipeline con información GDPR completa.\n    \"\"\"\n    # Agrupar por tipo de detección\n    by_type = {}\n    for track in detection_result.detections:\n        t = track.detection_type\n        if t not in by_type:\n            by_type[t] = {\"count\": 0, \"violations\": 0}\n        by_type[t][\"count\"] += 1\n    \n    # Contar violaciones por tipo\n    for v in violations:\n        t = v[\"track\"].detection_type\n        if t in by_type:\n            by_type[t][\"violations\"] += 1\n    \n    # Construir estructura de violaciones\n    violations_list = []\n    for i, v in enumerate(violations, 1):\n        track = v[\"track\"]\n        verification = v.get(\"verification\", {})\n        \n        # Calcular rango de frames\n        first_frame = track.bbox_history[0].frame if track.bbox_history else 0\n        last_frame = track.bbox_history[-1].frame if track.bbox_history else 0\n        duration = (last_frame - first_frame) / detection_result.fps if detection_result.fps > 0 else 0\n        \n        # Mejor captura\n        best_capture_path = None\n        if track.best_capture:\n            best_capture_path = track.best_capture.image_path\n        \n        # Avg confidence del track\n        avg_confidence = 0.0\n        if track.bbox_history:\n            avg_confidence = sum(b.confidence for b in track.bbox_history) / len(track.bbox_history)\n        \n        violations_list.append({\n            \"id\": f\"viol_{i:03d}\",\n            \"track_id\": track.track_id,\n            \"detection_type\": track.detection_type,\n            \"severity\": v.get(\"severity\", \"medium\"),\n            \"confidence\": verification.get(\"confidence\", avg_confidence),\n            \"violated_articles\": v.get(\"articles\", []),\n            \"description\": verification.get(\"description\", f\"Detected {track.detection_type} which constitutes personal data.\"),\n            \"reasoning\": verification.get(\"reasoning\", \"\"),\n            \"recommended_action\": v.get(\"action\", \"blur\"),\n            \"action_applied\": v.get(\"action\", \"blur\"),\n            \"capture_image\": best_capture_path,\n            \"frame_range\": {\n                \"first\": first_frame,\n                \"last\": last_frame\n            },\n            \"duration_seconds\": round(duration, 2),\n            \"total_frames\": len(track.bbox_history)\n        })\n    \n    # Determinar si se requiere consentimiento\n    has_high_severity = any(v.get(\"severity\") == \"high\" for v in violations)\n    has_biometric = any(\"face\" in v[\"track\"].detection_type for v in violations)\n    \n    result = {\n        \"video_id\": video_id,\n        \"processing_date\": datetime.utcnow().isoformat() + \"Z\",\n        \"pipeline_version\": \"1.0.0\",\n        \"architecture\": \"Hybrid (Kornia + YOLO)\",\n        \"verification_method\": \"Neo4j + Gemma 3n\" if VERIFICATION_MODULE_AVAILABLE else \"Hardcoded GDPR_CONFIG\",\n        \n        \"video_metadata\": {\n            \"original_path\": detection_result.video_path,\n            \"output_path\": str(output_path) if output_path else None,\n            \"duration_seconds\": round(detection_result.duration_seconds, 2),\n            \"fps\": detection_result.fps,\n            \"resolution\": {\n                \"width\": detection_result.width,\n                \"height\": detection_result.height\n            },\n            \"total_frames\": detection_result.total_frames\n        },\n        \n        \"processing_stats\": {\n            \"processing_time_seconds\": round(processing_time, 2),\n            \"frames_processed\": detection_result.frames_processed,\n            \"detections_total\": len(detection_result.detections),\n            \"violations_total\": len(violations)\n        },\n        \n        \"detections_summary\": by_type,\n        \n        \"violations\": violations_list,\n        \n        \"gdpr_compliance\": {\n            \"requires_consent\": has_high_severity or has_biometric,\n            \"legal_basis_required\": \"consent\" if (has_high_severity or has_biometric) else \"legitimate_interest\",\n            \"contains_biometric_data\": has_biometric,\n            \"data_categories\": list(by_type.keys()),\n            \"data_retention_recommendation\": \"30 days\",\n            \"anonymization_applied\": True,\n            \"applicable_articles\": list(set(\n                article \n                for v in violations \n                for article in v.get(\"articles\", [])\n            ))\n        }\n    }\n    \n    return result\n\n\n# =============================================================================\n# VIDEO PROCESSOR - PIPELINE INTEGRADO (Arquitectura Híbrida)\n# =============================================================================\nclass VideoProcessor:\n    \"\"\"\n    Orquestador del pipeline completo de anonimización GDPR.\n    \n    Integra:\n    - HybridDetectorManager: Kornia FaceDetector (YuNet) + YOLOv10\n    - Módulo de Verificación: Neo4j + Gemma 3n (cuando está disponible)\n    - KorniaEffects: Efectos GPU acelerados\n    - Generador de JSON: Formulario de resultados GDPR\n    \"\"\"\n    \n    def __init__(self, person_model: str = None, plate_model: str = None):\n        self.progress = MockProgressManager()\n        self.db = MockDB()\n        self.gpu = gpu_manager\n        \n        # Gestor de detección híbrido\n        self.detector = HybridDetectorManager(\n            gpu_mgr=self.gpu,\n            person_model=person_model,\n            plate_model=plate_model\n        )\n        \n        self.batch_size = self.detector.batch_size\n        \n        # Efectos Kornia\n        self.effects = kornia_effects\n        \n        logger.info(f\"Pipeline initialized: {self.detector.get_info()}\")\n    \n    async def _verify_with_module(self, track: TrackedDetection, captures_dir: Path) -> Dict[str, Any]:\n        \"\"\"\n        Verifica una detección usando el módulo de verificación (Neo4j + Gemma 3n).\n        \"\"\"\n        # Buscar imagen de captura para este track\n        image_path = None\n        if track.best_capture:\n            image_path = track.best_capture.image_path\n        \n        if not image_path:\n            # Buscar en directorio de capturas\n            track_dir = captures_dir / f\"track_{track.track_id}\"\n            if track_dir.exists():\n                images = list(track_dir.glob(\"*.jpg\"))\n                if images:\n                    image_path = str(images[0])\n        \n        if not image_path or not os.path.exists(image_path):\n            # Sin imagen, usar verificación fallback\n            return await self._verify_with_fallback(track)\n        \n        # Preparar detección para el módulo\n        detection_data = {\n            \"id\": track.track_id,\n            \"detection_type\": track.detection_type,\n            \"confidence\": track.bbox_history[0].confidence if track.bbox_history else 0.5,\n            \"bbox\": track.bbox_history[0].to_dict() if track.bbox_history else {}\n        }\n        \n        try:\n            results = await verify_image_detections(image_path, [detection_data])\n            if results and len(results) > 0:\n                result = results[0]\n                return {\n                    \"is_violation\": result.get(\"is_violation\", True),\n                    \"severity\": result.get(\"severity\", \"medium\"),\n                    \"articles\": result.get(\"violated_articles\", [\"6\"]),\n                    \"action\": result.get(\"recommended_action\", \"blur\"),\n                    \"verification\": result\n                }\n        except Exception as e:\n            logger.warning(f\"Error en verificación con módulo: {e}, usando fallback\")\n        \n        return await self._verify_with_fallback(track)\n    \n    async def _verify_with_fallback(self, track: TrackedDetection) -> Dict[str, Any]:\n        \"\"\"\n        Verificación fallback usando GDPR_CONFIG hardcodeado.\n        \"\"\"\n        gdpr_info = GDPR_CONFIG.get(track.detection_type, \n            {\"severity\": \"low\", \"action\": \"blur\", \"articles\": [\"6\"]})\n        \n        return {\n            \"is_violation\": True,\n            \"severity\": gdpr_info[\"severity\"],\n            \"articles\": gdpr_info[\"articles\"],\n            \"action\": gdpr_info[\"action\"],\n            \"verification\": {\n                \"is_violation\": True,\n                \"severity\": gdpr_info[\"severity\"],\n                \"violated_articles\": gdpr_info[\"articles\"],\n                \"description\": f\"Detected {track.detection_type} which constitutes personal data.\",\n                \"recommended_action\": gdpr_info[\"action\"],\n                \"confidence\": 0.9\n            }\n        }\n    \n    async def process_full_pipeline(self, video_id: str, input_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Pipeline completo: Detección Híbrida → Verificación GDPR → JSON → Anonimización\n        \n        Returns:\n            Dict con el JSON de resultados generado\n        \"\"\"\n        start_time = time.time()\n        output_json = None\n        \n        try:\n            await self.progress.register_video(video_id)\n            \n            # ═══════════════════════════════════════════════════════════════\n            # FASE 1: DETECCIÓN HÍBRIDA (Kornia + YOLO)\n            # ═══════════════════════════════════════════════════════════════\n            info = self.detector.get_info()\n            await self.progress.change_phase(video_id, ProcessingPhase.DETECTING, \n                f\"Detección híbrida: {', '.join(info['detectors'])}\")\n            \n            output_dir = Path(\"../storage/captures\") / video_id\n            output_dir.mkdir(parents=True, exist_ok=True)\n            \n            detection_result = await self._detect_video(input_path, output_dir, video_id)\n            \n            # Resumen de detecciones\n            by_type = {}\n            for det in detection_result.detections:\n                by_type.setdefault(det.detection_type, []).append(det)\n            \n            print(f\"\\n  Detecciones: {dict((k, len(v)) for k, v in by_type.items())}\")\n            \n            # ═══════════════════════════════════════════════════════════════\n            # FASE 2: VERIFICACIÓN GDPR (Neo4j + Gemma 3n o fallback)\n            # ═══════════════════════════════════════════════════════════════\n            verification_method = \"Neo4j + Gemma 3n\" if VERIFICATION_MODULE_AVAILABLE else \"GDPR_CONFIG fallback\"\n            await self.progress.change_phase(video_id, ProcessingPhase.VERIFYING,\n                f\"Verificando cumplimiento GDPR ({verification_method})...\")\n            \n            violations = []\n            total_tracks = len(detection_result.detections)\n            \n            for idx, track in enumerate(detection_result.detections):\n                if VERIFICATION_MODULE_AVAILABLE:\n                    v_result = await self._verify_with_module(track, output_dir)\n                else:\n                    v_result = await self._verify_with_fallback(track)\n                \n                if v_result.get(\"is_violation\", False):\n                    violations.append({\n                        \"track\": track,\n                        \"severity\": v_result[\"severity\"],\n                        \"action\": v_result[\"action\"],\n                        \"articles\": v_result[\"articles\"],\n                        \"verification\": v_result.get(\"verification\", {})\n                    })\n                \n                # Progreso de verificación\n                if (idx + 1) % 10 == 0 or idx == total_tracks - 1:\n                    pct = int((idx + 1) / total_tracks * 100)\n                    await self.progress.update_progress(video_id, pct, idx + 1, total_tracks,\n                        f\"Verificando track {idx + 1}/{total_tracks}\")\n            \n            print(f\"  Violaciones GDPR: {len(violations)}\")\n            print(f\"  Método de verificación: {verification_method}\")\n            \n            # ═══════════════════════════════════════════════════════════════\n            # FASE 3: GENERACIÓN DE JSON DE RESULTADOS\n            # ═══════════════════════════════════════════════════════════════\n            processing_time = time.time() - start_time\n            output_path = Path(f\"../storage/processed/{video_id}_anonymized.mp4\")\n            \n            output_json = generate_output_json(\n                video_id=video_id,\n                detection_result=detection_result,\n                violations=violations,\n                output_path=output_path,\n                processing_time=processing_time\n            )\n            \n            # Guardar JSON\n            json_path = Path(f\"../storage/processed/{video_id}_results.json\")\n            json_path.parent.mkdir(parents=True, exist_ok=True)\n            \n            with open(json_path, 'w', encoding='utf-8') as f:\n                json.dump(output_json, f, indent=2, ensure_ascii=False)\n            \n            print(f\"\\n  JSON guardado: {json_path}\")\n            \n            # ═══════════════════════════════════════════════════════════════\n            # FASE 4: ANONIMIZACIÓN (Kornia GPU)\n            # ═══════════════════════════════════════════════════════════════\n            if violations:\n                await self.progress.change_phase(video_id, ProcessingPhase.EDITING,\n                    f\"Anonimizando {len(violations)} elementos con Kornia GPU...\")\n                \n                await self._anonymize_video(input_path, str(output_path), violations, video_id)\n                \n                # Actualizar JSON con confirmación de anonimización\n                output_json[\"video_metadata\"][\"output_path\"] = str(output_path)\n                output_json[\"gdpr_compliance\"][\"anonymization_applied\"] = True\n                output_json[\"processing_stats\"][\"processing_time_seconds\"] = round(time.time() - start_time, 2)\n                \n                # Re-guardar JSON actualizado\n                with open(json_path, 'w', encoding='utf-8') as f:\n                    json.dump(output_json, f, indent=2, ensure_ascii=False)\n            \n            # ═══════════════════════════════════════════════════════════════\n            # COMPLETADO\n            # ═══════════════════════════════════════════════════════════════\n            elapsed = time.time() - start_time\n            \n            summary = {\n                \"architecture\": \"Hybrid (Kornia + YOLO)\",\n                \"verification\": verification_method,\n                \"detectors\": info['detectors'],\n                \"total_detections\": len(detection_result.detections),\n                \"total_violations\": len(violations),\n                \"by_type\": {k: len(v) for k, v in by_type.items()},\n                \"processing_time\": f\"{elapsed:.2f}s\",\n                \"output_video\": str(output_path) if violations else None,\n                \"output_json\": str(json_path)\n            }\n            \n            await self.progress.complete(video_id, **summary)\n            \n            return output_json\n            \n        except Exception as e:\n            import traceback\n            traceback.print_exc()\n            await self.progress.error(video_id, \"PIPELINE_ERROR\", str(e))\n            return None\n    \n    async def _detect_video(self, video_path: str, output_dir: Path, video_id: str) -> DetectionResult:\n        \"\"\"Fase de detección con HybridDetectorManager\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        tracker = ObjectTracker()\n        tracked = {}\n        frame_num = 0\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_num += 1\n            \n            # Detectar con HybridDetectorManager (Kornia + YOLO)\n            detections = self.detector.detect_all(frame, frame_num)\n            \n            # Actualizar tracker\n            confirmed = tracker.update(detections, frame_num)\n            \n            for tid, cls, bbox in confirmed:\n                if tid not in tracked:\n                    tracked[tid] = TrackedDetection(tid, cls)\n                tracked[tid].add_bbox(bbox)\n            \n            # Progreso\n            if frame_num % 30 == 0:\n                pct = int(frame_num / total_frames * 100)\n                await self.progress.update_progress(video_id, pct, frame_num, total_frames, \n                    f\"Frame {frame_num}/{total_frames}\")\n        \n        cap.release()\n        \n        return DetectionResult(\n            video_path=video_path,\n            total_frames=total_frames,\n            fps=fps,\n            duration_seconds=total_frames/fps if fps > 0 else 0,\n            width=width,\n            height=height,\n            detections=list(tracked.values()),\n            frames_processed=frame_num\n        )\n    \n    async def _anonymize_video(self, input_path: str, output_path: str, violations: List[Dict], video_id: str):\n        \"\"\"Fase de anonimización con Kornia GPU\"\"\"\n        cap = cv2.VideoCapture(input_path)\n        \n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n        \n        # Construir mapa de acciones por frame\n        actions_by_frame = {}\n        for v in violations:\n            track = v[\"track\"]\n            for bbox in track.bbox_history:\n                if bbox.frame not in actions_by_frame:\n                    actions_by_frame[bbox.frame] = []\n                actions_by_frame[bbox.frame].append({\n                    \"bbox\": (int(bbox.x1), int(bbox.y1), int(bbox.x2), int(bbox.y2)),\n                    \"action\": v[\"action\"],\n                    \"track_id\": track.track_id\n                })\n        \n        frame_num = 0\n        use_gpu = self.effects is not None and KORNIA_AVAILABLE\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_num += 1\n            \n            if frame_num in actions_by_frame:\n                if use_gpu:\n                    tensor = self.effects.numpy_to_tensor(frame)\n                    \n                    for action in actions_by_frame[frame_num]:\n                        bbox = action[\"bbox\"]\n                        if action[\"action\"] == \"blur\":\n                            tensor = self.effects.blur_region(tensor, bbox)\n                        elif action[\"action\"] == \"pixelate\":\n                            tensor = self.effects.pixelate_region(tensor, bbox, blocks=15, track_id=action[\"track_id\"])\n                    \n                    frame = self.effects.tensor_to_numpy(tensor)\n                else:\n                    for action in actions_by_frame[frame_num]:\n                        x1, y1, x2, y2 = action[\"bbox\"]\n                        roi = frame[y1:y2, x1:x2]\n                        if action[\"action\"] == \"blur\":\n                            k = 31\n                            frame[y1:y2, x1:x2] = cv2.GaussianBlur(roi, (k, k), 0)\n                        elif action[\"action\"] == \"pixelate\":\n                            small = cv2.resize(roi, (15, 15))\n                            frame[y1:y2, x1:x2] = cv2.resize(small, (x2-x1, y2-y1), interpolation=cv2.INTER_NEAREST)\n            \n            out.write(frame)\n            \n            if frame_num % 30 == 0:\n                pct = int(frame_num / total_frames * 100)\n                await self.progress.update_progress(video_id, pct, frame_num, total_frames,\n                    f\"Anonimizando {frame_num}/{total_frames}\")\n        \n        cap.release()\n        out.release()\n        \n        print(f\"  Video guardado: {output_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ejecución del Pipeline\n",
    "Lanzamos el proceso completo sobre el video de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EJECUCIÓN DEL PIPELINE - ARQUITECTURA HÍBRIDA CON VERIFICACIÓN GDPR\n# =============================================================================\n\nVIDEO_ID = \"test_pipeline_hybrid\"\nVIDEO_PATH = \"../storage/uploads/coche.mp4\"\n\n# Modelos personalizados (opcional)\nCUSTOM_PERSON_MODEL = None  # e.g., \"../models/yolov10m.pt\"\nCUSTOM_PLATE_MODEL = None   # e.g., \"../models/yolo-lpr.pt\"\n\n# Variable global para acceder al JSON de resultados\npipeline_result_json = None\n\nasync def run_pipeline():\n    global pipeline_result_json\n    \n    print(\"=\" * 70)\n    print(\"OCCULTASHIELD - Pipeline de Anonimización GDPR\")\n    print(\"Arquitectura Híbrida: Kornia FaceDetector + YOLOv10\")\n    print(\"Verificación: Neo4j + Gemma 3n\" if VERIFICATION_MODULE_AVAILABLE else \"Verificación: Fallback GDPR_CONFIG\")\n    print(\"=\" * 70)\n    \n    # Mostrar configuración\n    strategy, model_size, batch_size = gpu_manager.get_strategy()\n    print(f\"\\nConfiguración auto-adaptativa:\")\n    print(f\"  Device: {gpu_manager.device_name}\")\n    print(f\"  VRAM: {gpu_manager.vram_total_mb}MB\")\n    print(f\"  Estrategia: {strategy}\")\n    print(f\"  Tamaño modelos: {model_size}\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Kornia FaceDetector: {'Sí' if KORNIA_FACE_AVAILABLE else 'No (OpenCV fallback)'}\")\n    print(f\"  Kornia Effects: {'Sí' if KORNIA_AVAILABLE else 'No (CPU fallback)'}\")\n    print(f\"  Verificación GDPR: {'Neo4j + Gemma 3n' if VERIFICATION_MODULE_AVAILABLE else 'GDPR_CONFIG fallback'}\")\n    print()\n    \n    if not os.path.exists(VIDEO_PATH):\n        print(f\"[!] Video no encontrado: {VIDEO_PATH}\")\n        print(\"    Descarga un video de prueba para ejecutar el pipeline.\")\n        return None\n    \n    processor = VideoProcessor(\n        person_model=CUSTOM_PERSON_MODEL,\n        plate_model=CUSTOM_PLATE_MODEL\n    )\n    \n    # Ejecutar pipeline y obtener JSON de resultados\n    pipeline_result_json = await processor.process_full_pipeline(VIDEO_ID, VIDEO_PATH)\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ARQUITECTURA DEL PIPELINE\")\n    print(\"=\" * 70)\n    print(\"\"\"\n┌─────────────────────────────────────────────────────────────────────┐\n│                    PIPELINE COMPLETO OCCULTASHIELD                  │\n├─────────────────────────────────────────────────────────────────────┤\n│                                                                     │\n│  FASE 1: DETECCIÓN                                                  │\n│  ├─ Caras: Kornia FaceDetector (YuNet) - GPU nativo                 │\n│  ├─ Personas: YOLOv10 (nano/s/m según VRAM)                         │\n│  └─ Matrículas: YOLO-LPR (opcional)                                 │\n│                                                                     │\n│  FASE 2: VERIFICACIÓN GDPR                                          │\n│  ├─ GraphRAG: Neo4j con artículos GDPR                              │\n│  ├─ LLM: Gemma 3n para análisis contextual                          │\n│  └─ Fallback: GDPR_CONFIG si módulo no disponible                   │\n│                                                                     │\n│  FASE 3: GENERACIÓN JSON                                            │\n│  ├─ Formulario de resultados completo                               │\n│  ├─ Violaciones con artículos GDPR                                  │\n│  └─ Recomendaciones de cumplimiento                                 │\n│                                                                     │\n│  FASE 4: ANONIMIZACIÓN                                              │\n│  ├─ KorniaEffects: Blur/Pixelate GPU acelerado                      │\n│  └─ Tracking consistente por ID                                     │\n│                                                                     │\n└─────────────────────────────────────────────────────────────────────┘\n\nArchivos de salida:\n  - Video: ../storage/processed/{video_id}_anonymized.mp4\n  - JSON:  ../storage/processed/{video_id}_results.json\n    \"\"\")\n    \n    return pipeline_result_json\n\n# Ejecutar\nresult = asyncio.run(run_pipeline())\n\n# Mostrar resumen del JSON si está disponible\nif result:\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RESUMEN JSON DE RESULTADOS\")\n    print(\"=\" * 70)\n    print(f\"  Video ID: {result.get('video_id')}\")\n    print(f\"  Fecha: {result.get('processing_date')}\")\n    print(f\"  Verificación: {result.get('verification_method')}\")\n    print(f\"  Detecciones: {result.get('processing_stats', {}).get('detections_total', 0)}\")\n    print(f\"  Violaciones: {result.get('processing_stats', {}).get('violations_total', 0)}\")\n    print(f\"  Tiempo: {result.get('processing_stats', {}).get('processing_time_seconds', 0):.2f}s\")\n    print(f\"\\n  Artículos GDPR aplicables: {result.get('gdpr_compliance', {}).get('applicable_articles', [])}\")\n    print(f\"  Requiere consentimiento: {result.get('gdpr_compliance', {}).get('requires_consent', False)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "l56ej4xw99n",
   "source": "### 5. Ejecutar Módulo de Edición Independientemente\n\nEsta celda permite ejecutar el módulo de edición (02) de forma independiente usando el JSON de resultados generado. Útil para:\n- Re-procesar el video con diferentes configuraciones de efectos\n- Aplicar anonimización después de revisar el JSON manualmente\n- Testing y desarrollo del módulo de edición",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xsnqj1roktk",
   "source": "# =============================================================================\n# EJECUTAR MÓDULO DE EDICIÓN (02) INDEPENDIENTEMENTE\n# =============================================================================\n# Esta celda permite re-ejecutar la anonimización usando el JSON de resultados\n\nasync def run_edition_from_json(json_path: str, video_path: str = None, output_path: str = None):\n    \"\"\"\n    Ejecuta el módulo de edición usando un JSON de resultados existente.\n    \n    Args:\n        json_path: Ruta al JSON de resultados del pipeline\n        video_path: Ruta al video original (opcional, se lee del JSON si no se proporciona)\n        output_path: Ruta de salida (opcional, genera nueva si no se proporciona)\n    \n    Returns:\n        Ruta del video procesado\n    \"\"\"\n    # Cargar JSON de resultados\n    with open(json_path, 'r', encoding='utf-8') as f:\n        result_json = json.load(f)\n    \n    # Obtener paths\n    if video_path is None:\n        video_path = result_json[\"video_metadata\"][\"original_path\"]\n    \n    if output_path is None:\n        video_id = result_json[\"video_id\"]\n        output_path = f\"../storage/processed/{video_id}_reedited.mp4\"\n    \n    print(\"=\" * 70)\n    print(\"MÓDULO DE EDICIÓN - Procesamiento desde JSON\")\n    print(\"=\" * 70)\n    print(f\"  Video entrada: {video_path}\")\n    print(f\"  Video salida: {output_path}\")\n    print(f\"  Violaciones a procesar: {len(result_json['violations'])}\")\n    print()\n    \n    # Verificar que el video existe\n    if not os.path.exists(video_path):\n        print(f\"[!] Video no encontrado: {video_path}\")\n        return None\n    \n    # Configuración de efectos por tipo de detección\n    EFFECT_CONFIG = {\n        \"face\": {\"action\": \"blur\", \"kernel_size\": 31, \"sigma\": 15.0},\n        \"person\": {\"action\": \"blur\", \"kernel_size\": 25, \"sigma\": 12.0},\n        \"license_plate\": {\"action\": \"pixelate\", \"blocks\": 10},\n        \"fingerprint\": {\"action\": \"pixelate\", \"blocks\": 8},\n        \"id_document\": {\"action\": \"blur\", \"kernel_size\": 41, \"sigma\": 20.0},\n        \"credit_card\": {\"action\": \"pixelate\", \"blocks\": 12},\n        \"signature\": {\"action\": \"blur\", \"kernel_size\": 21, \"sigma\": 10.0},\n    }\n    \n    # Cargar video\n    cap = cv2.VideoCapture(video_path)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Preparar writer\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    # Construir mapa de acciones por frame desde JSON\n    # Nota: El JSON almacena frame_range pero no bboxes individuales por frame\n    # Para un uso completo, necesitaríamos reejecutar la detección o almacenar más datos\n    # Aquí hacemos una versión simplificada que aplica el efecto en todo el rango\n    \n    print(\"  [!] Nota: Esta versión simplificada aplica efectos en el rango de frames\")\n    print(\"      Para bboxes precisos por frame, usar el pipeline completo\")\n    print()\n    \n    # Procesar frames\n    frame_num = 0\n    effects = KorniaEffects() if KORNIA_AVAILABLE else None\n    \n    # Para esta versión, no tenemos bboxes precisos por frame en el JSON\n    # El JSON solo tiene frame_range. Para una implementación completa,\n    # habría que almacenar también los bboxes por frame o re-detectar.\n    \n    # Esta celda está preparada para cuando se extienda el JSON con más datos\n    print(\"  Procesando video...\")\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frame_num += 1\n        \n        # Escribir frame (sin modificar en esta versión simplificada)\n        # En una versión completa, aquí se aplicarían los efectos\n        out.write(frame)\n        \n        if frame_num % 100 == 0:\n            pct = int(frame_num / total_frames * 100)\n            print(f\"    Progreso: {pct}% ({frame_num}/{total_frames})\")\n    \n    cap.release()\n    out.release()\n    \n    print(f\"\\n  [✓] Video guardado: {output_path}\")\n    print(\"\\n  Para anonimización completa con bboxes precisos, ejecutar el pipeline completo\")\n    \n    return output_path\n\n# =============================================================================\n# EJEMPLO DE USO\n# =============================================================================\n# Descomentar para ejecutar la edición desde un JSON existente:\n\n# json_result_path = f\"../storage/processed/{VIDEO_ID}_results.json\"\n# if os.path.exists(json_result_path):\n#     asyncio.run(run_edition_from_json(json_result_path))\n# else:\n#     print(f\"JSON no encontrado: {json_result_path}\")\n#     print(\"Ejecuta primero el pipeline completo (celda anterior)\")\n\nprint(\"Módulo de edición independiente cargado.\")\nprint(\"Para ejecutar: asyncio.run(run_edition_from_json('path/to/results.json'))\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}