{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Pipeline Completo - OccultaShield\n",
    "\n",
    "Este notebook integra todos los módulos optimizados para crear un pipeline completo de anonimización GDPR:\n",
    "\n",
    "**Arquitectura Híbrida (Kornia + YOLO):**\n",
    "1. **Detección de Caras**: Kornia FaceDetector (YuNet) - nativo GPU\n",
    "2. **Detección de Personas**: YOLOv10 (nano/s/m según VRAM)\n",
    "3. **Detección de Matrículas**: YOLO-LPR\n",
    "4. **Edición**: KorniaEffects para efectos acelerados por GPU\n",
    "5. **Verificación**: GemmaClient con clasificación LLM para huellas/documentos\n",
    "\n",
    "**Flujo:**\n",
    "```\n",
    "Video → Detección Híbrida → Verificación GDPR → Anonimización GPU → Video Procesado\n",
    "        (Kornia+YOLO)        (LLM + Graph)       (Kornia filters)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "Importamos los componentes de los otros módulos (asumiendo que los archivos .py existen y son estables). Si usasemos solo los notebooks, copiaríamos el código aquí, pero importar mantiene este notebook limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OCCULTASHIELD - Pipeline de Anonimización GDPR (Arquitectura Híbrida)\n",
      "======================================================================\n",
      "✓ PyTorch: 2.9.1+cu128\n",
      "✓ CUDA disponible: True\n",
      "✓ GPU: NVIDIA A10\n",
      "✓ VRAM: 24124MB\n",
      "✓ Kornia disponible: True\n",
      "✓ Kornia FaceDetector (YuNet): True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Kornia para efectos GPU y detección de caras (YuNet)\n",
    "try:\n",
    "    import kornia\n",
    "    import kornia.filters\n",
    "    from kornia.contrib import FaceDetector, FaceDetectorResult\n",
    "    KORNIA_AVAILABLE = True\n",
    "    KORNIA_FACE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        import kornia\n",
    "        import kornia.filters\n",
    "        KORNIA_AVAILABLE = True\n",
    "        KORNIA_FACE_AVAILABLE = False\n",
    "    except ImportError:\n",
    "        KORNIA_AVAILABLE = False\n",
    "        KORNIA_FACE_AVAILABLE = False\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('full_pipeline')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OCCULTASHIELD - Pipeline de Anonimización GDPR (Arquitectura Híbrida)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "print(f\"✓ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    vram_mb = torch.cuda.get_device_properties(0).total_memory // (1024**2)\n",
    "    print(f\"✓ VRAM: {vram_mb}MB\")\n",
    "print(f\"✓ Kornia disponible: {KORNIA_AVAILABLE}\")\n",
    "print(f\"✓ Kornia FaceDetector (YuNet): {KORNIA_FACE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mocks de Infraestructura\n",
    "Como corremos en local sin base de datos real (SurrealDB) ni backend web:\n",
    "- **`MockProgressManager`**: Imprime por consola en vez de enviar WebSockets al frontend.\n",
    "- **`MockDB`**: Simula guardar datos devolviendo un ID falso, sin conectar a nada.\n",
    "- **`ProcessingPhase`**: Constantes para saber en qué etapa estamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACIÓN GDPR Y MOCKS\n",
    "# =============================================================================\n",
    "\n",
    "# Mapeo de severidad y acciones GDPR\n",
    "GDPR_CONFIG = {\n",
    "    \"face\": {\"severity\": \"high\", \"action\": \"blur\", \"articles\": [\"9\", \"6\"]},\n",
    "    \"fingerprint\": {\"severity\": \"high\", \"action\": \"pixelate\", \"articles\": [\"9\", \"6\"]},\n",
    "    \"license_plate\": {\"severity\": \"high\", \"action\": \"pixelate\", \"articles\": [\"6\", \"17\"]},\n",
    "    \"id_document\": {\"severity\": \"high\", \"action\": \"blur\", \"articles\": [\"9\", \"6\", \"32\"]},\n",
    "    \"credit_card\": {\"severity\": \"high\", \"action\": \"pixelate\", \"articles\": [\"6\", \"32\"]},\n",
    "    \"signature\": {\"severity\": \"medium\", \"action\": \"blur\", \"articles\": [\"6\"]},\n",
    "    \"person\": {\"severity\": \"medium\", \"action\": \"blur\", \"articles\": [\"6\", \"13\"]},\n",
    "}\n",
    "\n",
    "class MockProgressManager:\n",
    "    \"\"\"Simula el gestor de progreso WebSocket del backend\"\"\"\n",
    "    async def register_video(self, video_id): \n",
    "        print(f\"\\n[PIPELINE] Iniciado: {video_id}\")\n",
    "    \n",
    "    async def change_phase(self, video_id, phase, msg, **kwargs): \n",
    "        print(f\"[FASE] {phase}: {msg}\")\n",
    "    \n",
    "    async def update_progress(self, video_id, pct, cur, tot, msg): \n",
    "        if cur % 30 == 0 or pct >= 100:\n",
    "            print(f\"  └─ Progreso: {pct:3d}% | {msg}\")\n",
    "    \n",
    "    async def complete(self, video_id, **kwargs): \n",
    "        print(f\"\\n[✓] COMPLETADO\")\n",
    "        for k, v in kwargs.items():\n",
    "            print(f\"    {k}: {v}\")\n",
    "    \n",
    "    async def error(self, video_id, code, msg, **kwargs): \n",
    "        print(f\"\\n[✗] ERROR: {msg}\")\n",
    "\n",
    "class MockDB:\n",
    "    \"\"\"Simula la base de datos\"\"\"\n",
    "    async def create(self, table, data):\n",
    "        return [{\"id\": f\"{table}:mock_{id(data)}\", **data}]\n",
    "\n",
    "class ProcessingPhase:\n",
    "    DETECTING = \"detecting\"\n",
    "    TRACKING = \"tracking\"\n",
    "    VERIFYING = \"verifying\"\n",
    "    EDITING = \"editing\"\n",
    "    COMPLETED = \"completed\"\n",
    "\n",
    "# =============================================================================\n",
    "# MODELOS DE DATOS (copiados de 01_detection_module para independencia)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    x1: float\n",
    "    y1: float\n",
    "    x2: float\n",
    "    y2: float\n",
    "    confidence: float\n",
    "    frame: int\n",
    "\n",
    "    @property\n",
    "    def area(self) -> float:\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class Capture:\n",
    "    frame: int\n",
    "    image_path: str\n",
    "    bbox: BoundingBox\n",
    "    reason: str\n",
    "    timestamp: float\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class TrackedDetection:\n",
    "    track_id: int\n",
    "    detection_type: str\n",
    "    bbox_history: List[BoundingBox] = field(default_factory=list)\n",
    "    captures: List[Capture] = field(default_factory=list)\n",
    "    is_confirmed: bool = False\n",
    "    \n",
    "    @property\n",
    "    def best_capture(self) -> Optional[Capture]:\n",
    "        if not self.captures:\n",
    "            return None\n",
    "        return max(self.captures, key=lambda c: c.bbox.confidence)\n",
    "    \n",
    "    def add_bbox(self, bbox: BoundingBox):\n",
    "        self.bbox_history.append(bbox)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"track_id\": self.track_id,\n",
    "            \"detection_type\": self.detection_type,\n",
    "            \"bbox_history\": [b.to_dict() for b in self.bbox_history],\n",
    "            \"captures\": [c.to_dict() for c in self.captures],\n",
    "            \"is_confirmed\": self.is_confirmed,\n",
    "            \"total_frames\": len(self.bbox_history)\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    video_path: str\n",
    "    total_frames: int\n",
    "    fps: float\n",
    "    duration_seconds: float\n",
    "    width: int\n",
    "    height: int\n",
    "    detections: List[TrackedDetection] = field(default_factory=list)\n",
    "    frames_processed: int = 0\n",
    "    processing_time_seconds: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Orquestador (`VideoProcessor`) - Arquitectura Híbrida\n",
    "\n",
    "Esta clase une todo el flujo usando **HybridDetectorManager** (Kornia + YOLO).\n",
    "\n",
    "**Componentes:**\n",
    "- **HybridDetectorManager**: Kornia FaceDetector (YuNet) + YOLOv10 para personas + YOLO-LPR para matrículas\n",
    "- **KorniaEffects**: Blur/Pixelate GPU acelerado\n",
    "- **GDPRConfig**: Acciones por tipo de detección\n",
    "\n",
    "**Flujo `process_full_pipeline`:**\n",
    "1. **Detectar**: Usa `HybridDetectorManager.detect_all()`:\n",
    "   - Caras → Kornia FaceDetector (YuNet)\n",
    "   - Personas → YOLOv10 (nano/s/m según VRAM)\n",
    "   - Matrículas → YOLO-LPR\n",
    "2. **Tracking**: Asigna IDs estables con `ObjectTracker`\n",
    "3. **Verificar GDPR**: Clasifica violaciones según severidad\n",
    "4. **Anonimizar**: Aplica efectos GPU con `KorniaEffects` (blur/pixelate)\n",
    "5. **Completar**: Reporta éxito con métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPU MANAGER\n",
    "# =============================================================================\n",
    "class GPUManager:\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self._initialized:\n",
    "            return\n",
    "        self._initialized = True\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "            self.device_name = torch.cuda.get_device_name(0)\n",
    "            self.vram_total_mb = torch.cuda.get_device_properties(0).total_memory // (1024**2)\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "            self.device_name = \"CPU\"\n",
    "            self.vram_total_mb = 0\n",
    "    \n",
    "    def get_strategy(self) -> Tuple[str, str, int]:\n",
    "        vram_gb = self.vram_total_mb / 1024\n",
    "        if vram_gb < 8:\n",
    "            return \"sequential\", \"nano\", 4\n",
    "        elif vram_gb < 16:\n",
    "            return \"parallel\", \"small\", 16\n",
    "        else:\n",
    "            return \"parallel\", \"medium\", min(64, int(vram_gb * 2))\n",
    "\n",
    "gpu_manager = GPUManager()\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID DETECTOR MANAGER - Kornia FaceDetector + YOLOv10\n",
    "# =============================================================================\n",
    "class HybridDetectorManager:\n",
    "    \"\"\"\n",
    "    Gestor híbrido de detectores: Kornia AI (caras) + YOLO (personas, matrículas).\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Caras: Kornia YuNet (FaceDetector) - nativo GPU\n",
    "    - Personas: YOLOv10 (nano/s/m según VRAM)\n",
    "    - Matrículas: YOLO-LPR\n",
    "    \"\"\"\n",
    "    \n",
    "    YOLO_CONFIGS = {\n",
    "        \"nano\": {\"person\": \"yolov10n.pt\", \"plate\": \"yolov8n.pt\"},\n",
    "        \"small\": {\"person\": \"yolov10s.pt\", \"plate\": \"yolov8s.pt\"},\n",
    "        \"medium\": {\"person\": \"yolov10m.pt\", \"plate\": \"yolov8m.pt\"},\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        gpu_mgr: GPUManager = None,\n",
    "        person_model: str = None,\n",
    "        plate_model: str = None,\n",
    "        face_confidence: float = 0.5,\n",
    "        person_confidence: float = 0.5\n",
    "    ):\n",
    "        self.gpu = gpu_mgr or gpu_manager\n",
    "        self.device = self.gpu.device\n",
    "        self.strategy, self.model_size, self.batch_size = self.gpu.get_strategy()\n",
    "        \n",
    "        self.face_confidence = face_confidence\n",
    "        self.person_confidence = person_confidence\n",
    "        \n",
    "        self._init_face_detector()\n",
    "        self._init_yolo_detectors(person_model, plate_model)\n",
    "        \n",
    "        logger.info(f\"HybridDetectorManager: strategy={self.strategy}, size={self.model_size}, \"\n",
    "                   f\"device={self.device}, kornia_face={KORNIA_FACE_AVAILABLE}\")\n",
    "    \n",
    "    def _init_face_detector(self):\n",
    "        \"\"\"Inicializa Kornia FaceDetector (YuNet)\"\"\"\n",
    "        self.face_detector = None\n",
    "        \n",
    "        if KORNIA_FACE_AVAILABLE:\n",
    "            try:\n",
    "                self.face_detector = FaceDetector().to(self.device)\n",
    "                logger.info(\"✓ Kornia FaceDetector (YuNet) loaded\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load Kornia FaceDetector: {e}\")\n",
    "        \n",
    "        # Fallback OpenCV Haar\n",
    "        if self.face_detector is None:\n",
    "            self.face_cascade = cv2.CascadeClassifier(\n",
    "                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "            )\n",
    "            logger.info(\"Using OpenCV Haar Cascade fallback for face detection\")\n",
    "    \n",
    "    def _init_yolo_detectors(self, person_model: str, plate_model: str):\n",
    "        \"\"\"Inicializa detectores YOLO para personas y matrículas\"\"\"\n",
    "        config = self.YOLO_CONFIGS[self.model_size]\n",
    "        \n",
    "        # Detector de personas (YOLOv10)\n",
    "        person_path = person_model or config[\"person\"]\n",
    "        try:\n",
    "            self.person_detector = YOLO(person_path)\n",
    "            logger.info(f\"✓ YOLO person detector loaded: {person_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load person model: {e}\")\n",
    "            self.person_detector = None\n",
    "        \n",
    "        # Detector de matrículas (opcional)\n",
    "        self.plate_detector = None\n",
    "        if plate_model and os.path.exists(plate_model):\n",
    "            try:\n",
    "                self.plate_detector = YOLO(plate_model)\n",
    "                logger.info(f\"✓ YOLO plate detector loaded: {plate_model}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load plate model: {e}\")\n",
    "    \n",
    "    def _numpy_to_tensor(self, frame: np.ndarray) -> torch.Tensor:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        tensor = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "        return tensor.to(self.device)\n",
    "    \n",
    "    def detect_faces_kornia(self, tensor: torch.Tensor, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n",
    "        \"\"\"Detecta caras usando Kornia FaceDetector (YuNet)\"\"\"\n",
    "        if self.face_detector is None:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            detections = self.face_detector(tensor)\n",
    "        \n",
    "        for det in detections:\n",
    "            if det.data.numel() == 0:\n",
    "                continue\n",
    "            try:\n",
    "                data = det.data.cpu().numpy()\n",
    "                for row in data:\n",
    "                    if len(row) >= 5:\n",
    "                        x, y, w, h, score = row[:5]\n",
    "                        if score >= self.face_confidence:\n",
    "                            x1, y1 = float(x), float(y)\n",
    "                            x2, y2 = float(x + w), float(y + h)\n",
    "                            bbox = BoundingBox(x1, y1, x2, y2, float(score), frame_num)\n",
    "                            if bbox.area >= 500:\n",
    "                                results.append((\"face\", bbox))\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error processing face detection: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def detect_faces_opencv(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n",
    "        \"\"\"Fallback: Detección de caras con OpenCV Haar Cascade\"\"\"\n",
    "        if not hasattr(self, 'face_cascade'):\n",
    "            return []\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "        \n",
    "        results = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            bbox = BoundingBox(float(x), float(y), float(x + w), float(y + h), 0.8, frame_num)\n",
    "            if bbox.area >= 500:\n",
    "                results.append((\"face\", bbox))\n",
    "        return results\n",
    "    \n",
    "    def detect_persons(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n",
    "        \"\"\"Detecta personas usando YOLOv10\"\"\"\n",
    "        if self.person_detector is None:\n",
    "            return []\n",
    "        \n",
    "        results_yolo = self.person_detector.predict(\n",
    "            frame, conf=self.person_confidence, verbose=False, device=self.device\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for r in results_yolo:\n",
    "            for box in r.boxes:\n",
    "                cls = int(box.cls[0])\n",
    "                if cls == 0:  # COCO class 0 = person\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                    bbox = BoundingBox(x1, y1, x2, y2, float(box.conf[0]), frame_num)\n",
    "                    if bbox.area >= 500:\n",
    "                        results.append((\"person\", bbox))\n",
    "        return results\n",
    "    \n",
    "    def detect_plates(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n",
    "        \"\"\"Detecta matrículas usando YOLO-LPR\"\"\"\n",
    "        if self.plate_detector is None:\n",
    "            return []\n",
    "        \n",
    "        results_yolo = self.plate_detector.predict(\n",
    "            frame, conf=self.person_confidence, verbose=False, device=self.device\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for r in results_yolo:\n",
    "            for box in r.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                bbox = BoundingBox(x1, y1, x2, y2, float(box.conf[0]), frame_num)\n",
    "                if bbox.area >= 100:\n",
    "                    results.append((\"license_plate\", bbox))\n",
    "        return results\n",
    "    \n",
    "    def detect_all(self, frame: np.ndarray, frame_num: int) -> List[Tuple[str, BoundingBox]]:\n",
    "        \"\"\"Ejecuta todos los detectores en un frame\"\"\"\n",
    "        all_detections = []\n",
    "        \n",
    "        # Personas (YOLOv10)\n",
    "        all_detections.extend(self.detect_persons(frame, frame_num))\n",
    "        \n",
    "        # Caras (Kornia o OpenCV fallback)\n",
    "        if self.face_detector is not None:\n",
    "            tensor = self._numpy_to_tensor(frame)\n",
    "            all_detections.extend(self.detect_faces_kornia(tensor, frame_num))\n",
    "        else:\n",
    "            all_detections.extend(self.detect_faces_opencv(frame, frame_num))\n",
    "        \n",
    "        # Matrículas (YOLO)\n",
    "        all_detections.extend(self.detect_plates(frame, frame_num))\n",
    "        \n",
    "        return all_detections\n",
    "    \n",
    "    def get_info(self) -> Dict:\n",
    "        detectors = []\n",
    "        if self.person_detector:\n",
    "            detectors.append(\"person (YOLOv10)\")\n",
    "        if self.face_detector:\n",
    "            detectors.append(\"face (Kornia YuNet)\")\n",
    "        elif hasattr(self, 'face_cascade'):\n",
    "            detectors.append(\"face (OpenCV Haar)\")\n",
    "        if self.plate_detector:\n",
    "            detectors.append(\"plate (YOLO)\")\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": self.strategy,\n",
    "            \"model_size\": self.model_size,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"device\": self.device,\n",
    "            \"vram_total_mb\": self.gpu.vram_total_mb,\n",
    "            \"detectors\": detectors,\n",
    "            \"kornia_available\": KORNIA_FACE_AVAILABLE\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# KORNIA EFFECTS (de 02_edition_module)\n",
    "# =============================================================================\n",
    "class KorniaEffects:\n",
    "    def __init__(self, device: str = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.noise_cache = {}\n",
    "    \n",
    "    def numpy_to_tensor(self, frame: np.ndarray) -> torch.Tensor:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        tensor = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0)\n",
    "        return tensor.float().div(255.0).to(self.device)\n",
    "    \n",
    "    def tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:\n",
    "        arr = tensor.squeeze(0).permute(1, 2, 0)\n",
    "        arr = arr.mul(255.0).clamp(0, 255).byte().cpu().numpy()\n",
    "        return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    def blur_region(self, tensor, bbox, kernel_size=31, sigma=15.0):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        result = tensor.clone()\n",
    "        roi = tensor[:, :, y1:y2, x1:x2]\n",
    "        \n",
    "        if KORNIA_AVAILABLE and roi.numel() > 0:\n",
    "            ks = kernel_size | 1\n",
    "            blurred = kornia.filters.gaussian_blur2d(roi, (ks, ks), (sigma, sigma))\n",
    "            result[:, :, y1:y2, x1:x2] = blurred\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def pixelate_region(self, tensor, bbox, blocks=10, track_id=0):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        result = tensor.clone()\n",
    "        roi = tensor[:, :, y1:y2, x1:x2]\n",
    "        \n",
    "        if roi.shape[2] < 2 or roi.shape[3] < 2:\n",
    "            return result\n",
    "        \n",
    "        small = F.interpolate(roi, size=(blocks, blocks), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        cache_key = (track_id, blocks)\n",
    "        if cache_key not in self.noise_cache:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(track_id * 1000)\n",
    "            self.noise_cache[cache_key] = torch.rand(1, 3, blocks, blocks, generator=gen, device=self.device) * 0.2 - 0.1\n",
    "        \n",
    "        small = (small + self.noise_cache[cache_key]).clamp(0, 1)\n",
    "        pixelated = F.interpolate(small, size=(y2-y1, x2-x1), mode='nearest')\n",
    "        result[:, :, y1:y2, x1:x2] = pixelated\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.noise_cache.clear()\n",
    "\n",
    "kornia_effects = KorniaEffects() if KORNIA_AVAILABLE else None\n",
    "\n",
    "# =============================================================================\n",
    "# TRACKER SIMPLIFICADO\n",
    "# =============================================================================\n",
    "class ObjectTracker:\n",
    "    def __init__(self, iou_threshold=0.3, max_age=30, min_hits=3):\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.max_age = max_age\n",
    "        self.min_hits = min_hits\n",
    "        self.tracks = {}\n",
    "        self.next_id = 1\n",
    "    \n",
    "    def update(self, detections, frame_num):\n",
    "        for t in self.tracks.values():\n",
    "            t['age'] += 1\n",
    "        \n",
    "        confirmed = []\n",
    "        for cls, bbox in detections:\n",
    "            matched = False\n",
    "            for tid, t in self.tracks.items():\n",
    "                if t['cls'] == cls and self._iou(t['bbox'], bbox) >= self.iou_threshold:\n",
    "                    t['bbox'] = bbox\n",
    "                    t['age'] = 0\n",
    "                    t['hits'] += 1\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                self.tracks[self.next_id] = {'cls': cls, 'bbox': bbox, 'age': 0, 'hits': 1}\n",
    "                self.next_id += 1\n",
    "        \n",
    "        dead = [tid for tid, t in self.tracks.items() if t['age'] > self.max_age]\n",
    "        for tid in dead:\n",
    "            del self.tracks[tid]\n",
    "        \n",
    "        for tid, t in self.tracks.items():\n",
    "            if t['hits'] >= self.min_hits:\n",
    "                confirmed.append((tid, t['cls'], t['bbox']))\n",
    "        \n",
    "        return confirmed\n",
    "    \n",
    "    def _iou(self, b1, b2):\n",
    "        x1 = max(b1.x1, b2.x1)\n",
    "        y1 = max(b1.y1, b2.y1)\n",
    "        x2 = min(b1.x2, b2.x2)\n",
    "        y2 = min(b1.y2, b2.y2)\n",
    "        if x2 < x1 or y2 < y1:\n",
    "            return 0\n",
    "        inter = (x2 - x1) * (y2 - y1)\n",
    "        union = b1.area + b2.area - inter\n",
    "        return inter / union if union > 0 else 0\n",
    "\n",
    "# =============================================================================\n",
    "# VIDEO PROCESSOR - PIPELINE INTEGRADO (Arquitectura Híbrida)\n",
    "# =============================================================================\n",
    "class VideoProcessor:\n",
    "    \"\"\"\n",
    "    Orquestador del pipeline completo de anonimización GDPR.\n",
    "    \n",
    "    Integra:\n",
    "    - HybridDetectorManager: Kornia FaceDetector (YuNet) + YOLOv10\n",
    "    - KorniaEffects: Efectos GPU acelerados\n",
    "    - GDPRConfig: Configuración de acciones por tipo de detección\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, person_model: str = None, plate_model: str = None):\n",
    "        self.progress = MockProgressManager()\n",
    "        self.db = MockDB()\n",
    "        self.gpu = gpu_manager\n",
    "        \n",
    "        # Gestor de detección híbrido\n",
    "        self.detector = HybridDetectorManager(\n",
    "            gpu_mgr=self.gpu,\n",
    "            person_model=person_model,\n",
    "            plate_model=plate_model\n",
    "        )\n",
    "        \n",
    "        self.batch_size = self.detector.batch_size\n",
    "        \n",
    "        # Efectos Kornia\n",
    "        self.effects = kornia_effects\n",
    "        \n",
    "        logger.info(f\"Pipeline initialized: {self.detector.get_info()}\")\n",
    "    \n",
    "    async def process_full_pipeline(self, video_id: str, input_path: str):\n",
    "        \"\"\"Pipeline completo: Detección Híbrida → Verificación → Anonimización\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            await self.progress.register_video(video_id)\n",
    "            \n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            # FASE 1: DETECCIÓN HÍBRIDA (Kornia + YOLO)\n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            info = self.detector.get_info()\n",
    "            await self.progress.change_phase(video_id, ProcessingPhase.DETECTING, \n",
    "                f\"Detección híbrida: {', '.join(info['detectors'])}\")\n",
    "            \n",
    "            output_dir = Path(\"../storage/captures\") / video_id\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            detection_result = await self._detect_video(input_path, output_dir, video_id)\n",
    "            \n",
    "            # Resumen de detecciones\n",
    "            by_type = {}\n",
    "            for det in detection_result.detections:\n",
    "                by_type.setdefault(det.detection_type, []).append(det)\n",
    "            \n",
    "            print(f\"\\n  Detecciones: {dict((k, len(v)) for k, v in by_type.items())}\")\n",
    "            \n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            # FASE 2: VERIFICACIÓN GDPR\n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            await self.progress.change_phase(video_id, ProcessingPhase.VERIFYING,\n",
    "                \"Verificando cumplimiento GDPR...\")\n",
    "            \n",
    "            violations = []\n",
    "            for track in detection_result.detections:\n",
    "                gdpr_info = GDPR_CONFIG.get(track.detection_type, \n",
    "                    {\"severity\": \"low\", \"action\": \"blur\", \"articles\": [\"6\"]})\n",
    "                \n",
    "                if track.detection_type in GDPR_CONFIG:\n",
    "                    violations.append({\n",
    "                        \"track\": track,\n",
    "                        \"severity\": gdpr_info[\"severity\"],\n",
    "                        \"action\": gdpr_info[\"action\"],\n",
    "                        \"articles\": gdpr_info[\"articles\"]\n",
    "                    })\n",
    "            \n",
    "            print(f\"  Violaciones GDPR: {len(violations)}\")\n",
    "            \n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            # FASE 3: ANONIMIZACIÓN (Kornia GPU)\n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            output_path = None\n",
    "            if violations:\n",
    "                await self.progress.change_phase(video_id, ProcessingPhase.EDITING,\n",
    "                    f\"Anonimizando {len(violations)} elementos con Kornia GPU...\")\n",
    "                \n",
    "                output_path = f\"../storage/processed/{video_id}_anonymized.mp4\"\n",
    "                await self._anonymize_video(input_path, output_path, violations, video_id)\n",
    "            \n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            # COMPLETADO\n",
    "            # ═══════════════════════════════════════════════════════════════\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            summary = {\n",
    "                \"architecture\": \"Hybrid (Kornia + YOLO)\",\n",
    "                \"detectors\": info['detectors'],\n",
    "                \"total_detections\": len(detection_result.detections),\n",
    "                \"total_violations\": len(violations),\n",
    "                \"by_type\": {k: len(v) for k, v in by_type.items()},\n",
    "                \"processing_time\": f\"{elapsed:.2f}s\",\n",
    "                \"output\": output_path\n",
    "            }\n",
    "            \n",
    "            await self.progress.complete(video_id, **summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            await self.progress.error(video_id, \"PIPELINE_ERROR\", str(e))\n",
    "    \n",
    "    async def _detect_video(self, video_path: str, output_dir: Path, video_id: str) -> DetectionResult:\n",
    "        \"\"\"Fase de detección con HybridDetectorManager\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        tracker = ObjectTracker()\n",
    "        tracked = {}\n",
    "        frame_num = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_num += 1\n",
    "            \n",
    "            # Detectar con HybridDetectorManager (Kornia + YOLO)\n",
    "            detections = self.detector.detect_all(frame, frame_num)\n",
    "            \n",
    "            # Actualizar tracker\n",
    "            confirmed = tracker.update(detections, frame_num)\n",
    "            \n",
    "            for tid, cls, bbox in confirmed:\n",
    "                if tid not in tracked:\n",
    "                    tracked[tid] = TrackedDetection(tid, cls)\n",
    "                tracked[tid].add_bbox(bbox)\n",
    "            \n",
    "            # Progreso\n",
    "            if frame_num % 30 == 0:\n",
    "                pct = int(frame_num / total_frames * 100)\n",
    "                await self.progress.update_progress(video_id, pct, frame_num, total_frames, \n",
    "                    f\"Frame {frame_num}/{total_frames}\")\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        return DetectionResult(\n",
    "            video_path=video_path,\n",
    "            total_frames=total_frames,\n",
    "            fps=fps,\n",
    "            duration_seconds=total_frames/fps if fps > 0 else 0,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            detections=list(tracked.values()),\n",
    "            frames_processed=frame_num\n",
    "        )\n",
    "    \n",
    "    async def _anonymize_video(self, input_path: str, output_path: str, violations: List[Dict], video_id: str):\n",
    "        \"\"\"Fase de anonimización con Kornia GPU\"\"\"\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        \n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # Construir mapa de acciones por frame\n",
    "        actions_by_frame = {}\n",
    "        for v in violations:\n",
    "            track = v[\"track\"]\n",
    "            for bbox in track.bbox_history:\n",
    "                if bbox.frame not in actions_by_frame:\n",
    "                    actions_by_frame[bbox.frame] = []\n",
    "                actions_by_frame[bbox.frame].append({\n",
    "                    \"bbox\": (int(bbox.x1), int(bbox.y1), int(bbox.x2), int(bbox.y2)),\n",
    "                    \"action\": v[\"action\"],\n",
    "                    \"track_id\": track.track_id\n",
    "                })\n",
    "        \n",
    "        frame_num = 0\n",
    "        use_gpu = self.effects is not None and KORNIA_AVAILABLE\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_num += 1\n",
    "            \n",
    "            if frame_num in actions_by_frame:\n",
    "                if use_gpu:\n",
    "                    tensor = self.effects.numpy_to_tensor(frame)\n",
    "                    \n",
    "                    for action in actions_by_frame[frame_num]:\n",
    "                        bbox = action[\"bbox\"]\n",
    "                        if action[\"action\"] == \"blur\":\n",
    "                            tensor = self.effects.blur_region(tensor, bbox)\n",
    "                        elif action[\"action\"] == \"pixelate\":\n",
    "                            tensor = self.effects.pixelate_region(tensor, bbox, blocks=15, track_id=action[\"track_id\"])\n",
    "                    \n",
    "                    frame = self.effects.tensor_to_numpy(tensor)\n",
    "                else:\n",
    "                    for action in actions_by_frame[frame_num]:\n",
    "                        x1, y1, x2, y2 = action[\"bbox\"]\n",
    "                        roi = frame[y1:y2, x1:x2]\n",
    "                        if action[\"action\"] == \"blur\":\n",
    "                            k = 31\n",
    "                            frame[y1:y2, x1:x2] = cv2.GaussianBlur(roi, (k, k), 0)\n",
    "                        elif action[\"action\"] == \"pixelate\":\n",
    "                            small = cv2.resize(roi, (15, 15))\n",
    "                            frame[y1:y2, x1:x2] = cv2.resize(small, (x2-x1, y2-y1), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            out.write(frame)\n",
    "            \n",
    "            if frame_num % 30 == 0:\n",
    "                pct = int(frame_num / total_frames * 100)\n",
    "                await self.progress.update_progress(video_id, pct, frame_num, total_frames,\n",
    "                    f\"Anonimizando {frame_num}/{total_frames}\")\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        \n",
    "        print(f\"  Video guardado: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ejecución del Pipeline\n",
    "Lanzamos el proceso completo sobre el video de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OCCULTASHIELD - Pipeline de Anonimización GDPR\n",
      "Arquitectura Híbrida: Kornia FaceDetector + YOLOv10\n",
      "======================================================================\n",
      "\n",
      "Configuración auto-adaptativa:\n",
      "  Device: NVIDIA A10\n",
      "  VRAM: 24124MB\n",
      "  Estrategia: parallel\n",
      "  Tamaño modelos: medium\n",
      "  Batch size: 47\n",
      "  Kornia FaceDetector: Sí\n",
      "  Kornia Effects: Sí\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:full_pipeline:✓ Kornia FaceDetector (YuNet) loaded\n",
      "INFO:full_pipeline:✓ YOLO person detector loaded: yolov10m.pt\n",
      "INFO:full_pipeline:HybridDetectorManager: strategy=parallel, size=medium, device=cuda, kornia_face=True\n",
      "INFO:full_pipeline:Pipeline initialized: {'strategy': 'parallel', 'model_size': 'medium', 'batch_size': 47, 'device': 'cuda', 'vram_total_mb': 24124, 'detectors': ['person (YOLOv10)', 'face (Kornia YuNet)'], 'kornia_available': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PIPELINE] Iniciado: test_pipeline_hybrid\n",
      "[FASE] detecting: Detección híbrida: person (YOLOv10), face (Kornia YuNet)\n",
      "  └─ Progreso:   1% | Frame 30/1886\n",
      "  └─ Progreso:   3% | Frame 60/1886\n",
      "  └─ Progreso:   4% | Frame 90/1886\n",
      "  └─ Progreso:   6% | Frame 120/1886\n",
      "  └─ Progreso:   7% | Frame 150/1886\n",
      "  └─ Progreso:   9% | Frame 180/1886\n",
      "  └─ Progreso:  11% | Frame 210/1886\n",
      "  └─ Progreso:  12% | Frame 240/1886\n",
      "  └─ Progreso:  14% | Frame 270/1886\n",
      "  └─ Progreso:  15% | Frame 300/1886\n",
      "  └─ Progreso:  17% | Frame 330/1886\n",
      "  └─ Progreso:  19% | Frame 360/1886\n",
      "  └─ Progreso:  20% | Frame 390/1886\n",
      "  └─ Progreso:  22% | Frame 420/1886\n",
      "  └─ Progreso:  23% | Frame 450/1886\n",
      "  └─ Progreso:  25% | Frame 480/1886\n",
      "  └─ Progreso:  27% | Frame 510/1886\n",
      "  └─ Progreso:  28% | Frame 540/1886\n",
      "  └─ Progreso:  30% | Frame 570/1886\n",
      "  └─ Progreso:  31% | Frame 600/1886\n",
      "  └─ Progreso:  33% | Frame 630/1886\n",
      "  └─ Progreso:  34% | Frame 660/1886\n",
      "  └─ Progreso:  36% | Frame 690/1886\n",
      "  └─ Progreso:  38% | Frame 720/1886\n",
      "  └─ Progreso:  39% | Frame 750/1886\n",
      "  └─ Progreso:  41% | Frame 780/1886\n",
      "  └─ Progreso:  42% | Frame 810/1886\n",
      "  └─ Progreso:  44% | Frame 840/1886\n",
      "  └─ Progreso:  46% | Frame 870/1886\n",
      "  └─ Progreso:  47% | Frame 900/1886\n",
      "  └─ Progreso:  49% | Frame 930/1886\n",
      "  └─ Progreso:  50% | Frame 960/1886\n",
      "  └─ Progreso:  52% | Frame 990/1886\n",
      "  └─ Progreso:  54% | Frame 1020/1886\n",
      "  └─ Progreso:  55% | Frame 1050/1886\n",
      "  └─ Progreso:  57% | Frame 1080/1886\n",
      "  └─ Progreso:  58% | Frame 1110/1886\n",
      "  └─ Progreso:  60% | Frame 1140/1886\n",
      "  └─ Progreso:  62% | Frame 1170/1886\n",
      "  └─ Progreso:  63% | Frame 1200/1886\n",
      "  └─ Progreso:  65% | Frame 1230/1886\n",
      "  └─ Progreso:  66% | Frame 1260/1886\n",
      "  └─ Progreso:  68% | Frame 1290/1886\n",
      "  └─ Progreso:  69% | Frame 1320/1886\n",
      "  └─ Progreso:  71% | Frame 1350/1886\n",
      "  └─ Progreso:  73% | Frame 1380/1886\n",
      "  └─ Progreso:  74% | Frame 1410/1886\n",
      "  └─ Progreso:  76% | Frame 1440/1886\n",
      "  └─ Progreso:  77% | Frame 1470/1886\n",
      "  └─ Progreso:  79% | Frame 1500/1886\n",
      "  └─ Progreso:  81% | Frame 1530/1886\n",
      "  └─ Progreso:  82% | Frame 1560/1886\n",
      "  └─ Progreso:  84% | Frame 1590/1886\n",
      "  └─ Progreso:  85% | Frame 1620/1886\n",
      "  └─ Progreso:  87% | Frame 1650/1886\n",
      "  └─ Progreso:  89% | Frame 1680/1886\n",
      "  └─ Progreso:  90% | Frame 1710/1886\n",
      "  └─ Progreso:  92% | Frame 1740/1886\n",
      "  └─ Progreso:  93% | Frame 1770/1886\n",
      "  └─ Progreso:  95% | Frame 1800/1886\n",
      "  └─ Progreso:  97% | Frame 1830/1886\n",
      "  └─ Progreso:  98% | Frame 1860/1886\n",
      "\n",
      "  Detecciones: {'face': 69, 'person': 2}\n",
      "[FASE] verifying: Verificando cumplimiento GDPR...\n",
      "  Violaciones GDPR: 71\n",
      "[FASE] editing: Anonimizando 71 elementos con Kornia GPU...\n",
      "  └─ Progreso:   1% | Anonimizando 30/1886\n",
      "  └─ Progreso:   3% | Anonimizando 60/1886\n",
      "  └─ Progreso:   4% | Anonimizando 90/1886\n",
      "  └─ Progreso:   6% | Anonimizando 120/1886\n",
      "  └─ Progreso:   7% | Anonimizando 150/1886\n",
      "  └─ Progreso:   9% | Anonimizando 180/1886\n",
      "  └─ Progreso:  11% | Anonimizando 210/1886\n",
      "  └─ Progreso:  12% | Anonimizando 240/1886\n",
      "  └─ Progreso:  14% | Anonimizando 270/1886\n",
      "  └─ Progreso:  15% | Anonimizando 300/1886\n",
      "  └─ Progreso:  17% | Anonimizando 330/1886\n",
      "  └─ Progreso:  19% | Anonimizando 360/1886\n",
      "  └─ Progreso:  20% | Anonimizando 390/1886\n",
      "  └─ Progreso:  22% | Anonimizando 420/1886\n",
      "  └─ Progreso:  23% | Anonimizando 450/1886\n",
      "  └─ Progreso:  25% | Anonimizando 480/1886\n",
      "  └─ Progreso:  27% | Anonimizando 510/1886\n",
      "  └─ Progreso:  28% | Anonimizando 540/1886\n",
      "  └─ Progreso:  30% | Anonimizando 570/1886\n",
      "  └─ Progreso:  31% | Anonimizando 600/1886\n",
      "  └─ Progreso:  33% | Anonimizando 630/1886\n",
      "  └─ Progreso:  34% | Anonimizando 660/1886\n",
      "  └─ Progreso:  36% | Anonimizando 690/1886\n",
      "  └─ Progreso:  38% | Anonimizando 720/1886\n",
      "  └─ Progreso:  39% | Anonimizando 750/1886\n",
      "  └─ Progreso:  41% | Anonimizando 780/1886\n",
      "  └─ Progreso:  42% | Anonimizando 810/1886\n",
      "  └─ Progreso:  44% | Anonimizando 840/1886\n",
      "  └─ Progreso:  46% | Anonimizando 870/1886\n",
      "  └─ Progreso:  47% | Anonimizando 900/1886\n",
      "  └─ Progreso:  49% | Anonimizando 930/1886\n",
      "  └─ Progreso:  50% | Anonimizando 960/1886\n",
      "  └─ Progreso:  52% | Anonimizando 990/1886\n",
      "  └─ Progreso:  54% | Anonimizando 1020/1886\n",
      "  └─ Progreso:  55% | Anonimizando 1050/1886\n",
      "  └─ Progreso:  57% | Anonimizando 1080/1886\n",
      "  └─ Progreso:  58% | Anonimizando 1110/1886\n",
      "  └─ Progreso:  60% | Anonimizando 1140/1886\n",
      "  └─ Progreso:  62% | Anonimizando 1170/1886\n",
      "  └─ Progreso:  63% | Anonimizando 1200/1886\n",
      "  └─ Progreso:  65% | Anonimizando 1230/1886\n",
      "  └─ Progreso:  66% | Anonimizando 1260/1886\n",
      "  └─ Progreso:  68% | Anonimizando 1290/1886\n",
      "  └─ Progreso:  69% | Anonimizando 1320/1886\n",
      "  └─ Progreso:  71% | Anonimizando 1350/1886\n",
      "  └─ Progreso:  73% | Anonimizando 1380/1886\n",
      "  └─ Progreso:  74% | Anonimizando 1410/1886\n",
      "  └─ Progreso:  76% | Anonimizando 1440/1886\n",
      "  └─ Progreso:  77% | Anonimizando 1470/1886\n",
      "  └─ Progreso:  79% | Anonimizando 1500/1886\n",
      "  └─ Progreso:  81% | Anonimizando 1530/1886\n",
      "  └─ Progreso:  82% | Anonimizando 1560/1886\n",
      "  └─ Progreso:  84% | Anonimizando 1590/1886\n",
      "  └─ Progreso:  85% | Anonimizando 1620/1886\n",
      "  └─ Progreso:  87% | Anonimizando 1650/1886\n",
      "  └─ Progreso:  89% | Anonimizando 1680/1886\n",
      "  └─ Progreso:  90% | Anonimizando 1710/1886\n",
      "  └─ Progreso:  92% | Anonimizando 1740/1886\n",
      "  └─ Progreso:  93% | Anonimizando 1770/1886\n",
      "  └─ Progreso:  95% | Anonimizando 1800/1886\n",
      "  └─ Progreso:  97% | Anonimizando 1830/1886\n",
      "  └─ Progreso:  98% | Anonimizando 1860/1886\n",
      "  Video guardado: ../storage/processed/test_pipeline_hybrid_anonymized.mp4\n",
      "\n",
      "[✓] COMPLETADO\n",
      "    architecture: Hybrid (Kornia + YOLO)\n",
      "    detectors: ['person (YOLOv10)', 'face (Kornia YuNet)']\n",
      "    total_detections: 71\n",
      "    total_violations: 71\n",
      "    by_type: {'face': 69, 'person': 2}\n",
      "    processing_time: 170.94s\n",
      "    output: ../storage/processed/test_pipeline_hybrid_anonymized.mp4\n",
      "\n",
      "======================================================================\n",
      "ARQUITECTURA HÍBRIDA DE DETECCIÓN\n",
      "======================================================================\n",
      "\n",
      "┌─────────────────┬────────────┬─────────────┬─────────────────────────┐\n",
      "│ Tipo            │ Severidad  │ Acción      │ Motor de Detección      │\n",
      "├─────────────────┼────────────┼─────────────┼─────────────────────────┤\n",
      "│ face            │ Alta       │ blur        │ Kornia YuNet            │\n",
      "│ fingerprint     │ Alta       │ pixelate    │ LLM (contexto mano)     │\n",
      "│ license_plate   │ Alta       │ pixelate    │ YOLO-LPR                │\n",
      "│ id_document     │ Alta       │ blur        │ LLM                     │\n",
      "│ credit_card     │ Alta       │ pixelate    │ LLM                     │\n",
      "│ signature       │ Media      │ blur        │ LLM                     │\n",
      "│ person          │ Media      │ blur        │ YOLOv10                 │\n",
      "└─────────────────┴────────────┴─────────────┴─────────────────────────┘\n",
      "\n",
      "Ventajas de la Arquitectura Híbrida:\n",
      "  - Kornia FaceDetector (YuNet): Nativo GPU, sin dependencias extra\n",
      "  - YOLOv10: Precisión comprobada para personas\n",
      "  - Auto-adaptación según VRAM disponible\n",
      "  - Reduce modelos YOLO de 3 a 2 (persona + matrícula)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EJECUCIÓN DEL PIPELINE - ARQUITECTURA HÍBRIDA\n",
    "# =============================================================================\n",
    "\n",
    "VIDEO_ID = \"test_pipeline_hybrid\"\n",
    "VIDEO_PATH = \"../storage/uploads/coche.mp4\"\n",
    "\n",
    "# Modelos personalizados (opcional)\n",
    "CUSTOM_PERSON_MODEL = None  # e.g., \"../models/yolov10m.pt\"\n",
    "CUSTOM_PLATE_MODEL = None   # e.g., \"../models/yolo-lpr.pt\"\n",
    "\n",
    "async def run_pipeline():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OCCULTASHIELD - Pipeline de Anonimización GDPR\")\n",
    "    print(\"Arquitectura Híbrida: Kornia FaceDetector + YOLOv10\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Mostrar configuración\n",
    "    strategy, model_size, batch_size = gpu_manager.get_strategy()\n",
    "    print(f\"\\nConfiguración auto-adaptativa:\")\n",
    "    print(f\"  Device: {gpu_manager.device_name}\")\n",
    "    print(f\"  VRAM: {gpu_manager.vram_total_mb}MB\")\n",
    "    print(f\"  Estrategia: {strategy}\")\n",
    "    print(f\"  Tamaño modelos: {model_size}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Kornia FaceDetector: {'Sí' if KORNIA_FACE_AVAILABLE else 'No (OpenCV fallback)'}\")\n",
    "    print(f\"  Kornia Effects: {'Sí' if KORNIA_AVAILABLE else 'No (CPU fallback)'}\")\n",
    "    print()\n",
    "    \n",
    "    if not os.path.exists(VIDEO_PATH):\n",
    "        print(f\"[!] Video no encontrado: {VIDEO_PATH}\")\n",
    "        print(\"    Descarga un video de prueba para ejecutar el pipeline.\")\n",
    "        return\n",
    "    \n",
    "    processor = VideoProcessor(\n",
    "        person_model=CUSTOM_PERSON_MODEL,\n",
    "        plate_model=CUSTOM_PLATE_MODEL\n",
    "    )\n",
    "    await processor.process_full_pipeline(VIDEO_ID, VIDEO_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ARQUITECTURA HÍBRIDA DE DETECCIÓN\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "┌─────────────────┬────────────┬─────────────┬─────────────────────────┐\n",
    "│ Tipo            │ Severidad  │ Acción      │ Motor de Detección      │\n",
    "├─────────────────┼────────────┼─────────────┼─────────────────────────┤\n",
    "│ face            │ Alta       │ blur        │ Kornia YuNet            │\n",
    "│ fingerprint     │ Alta       │ pixelate    │ LLM (contexto mano)     │\n",
    "│ license_plate   │ Alta       │ pixelate    │ YOLO-LPR                │\n",
    "│ id_document     │ Alta       │ blur        │ LLM                     │\n",
    "│ credit_card     │ Alta       │ pixelate    │ LLM                     │\n",
    "│ signature       │ Media      │ blur        │ LLM                     │\n",
    "│ person          │ Media      │ blur        │ YOLOv10                 │\n",
    "└─────────────────┴────────────┴─────────────┴─────────────────────────┘\n",
    "\n",
    "Ventajas de la Arquitectura Híbrida:\n",
    "  - Kornia FaceDetector (YuNet): Nativo GPU, sin dependencias extra\n",
    "  - YOLOv10: Precisión comprobada para personas\n",
    "  - Auto-adaptación según VRAM disponible\n",
    "  - Reduce modelos YOLO de 3 a 2 (persona + matrícula)\n",
    "    \"\"\")\n",
    "\n",
    "# Ejecutar\n",
    "asyncio.run(run_pipeline())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
